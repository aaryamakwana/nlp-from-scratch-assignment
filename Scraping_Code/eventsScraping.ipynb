{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defb7fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to event_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d\"\n",
    "\n",
    "# Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Prepare a list to store the event details\n",
    "events = []\n",
    "\n",
    "# Find all event containers\n",
    "for event in soup.find_all('a', href=True):\n",
    "    event_details = {}\n",
    "    \n",
    "    # Extract event name and URL\n",
    "    event_details['name'] = event.text.strip()\n",
    "    event_details['url'] = event['href']\n",
    "    \n",
    "    # Extract date, location, and description\n",
    "    date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "    location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "    description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "    event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "    event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "    event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "    \n",
    "    # Append the event details to the list\n",
    "    events.append(event_details)\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('event_data.json', 'w') as json_file:\n",
    "    json.dump(events, json_file, indent=4)\n",
    "\n",
    "print(\"Scraped data saved to event_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be42cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to event_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?v=d\"\n",
    "\n",
    "# Send a GET request to fetch the webpage content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Prepare a list to store the event details\n",
    "events = []\n",
    "\n",
    "# Find all event containers\n",
    "for event in soup.find_all('p', class_=\"fdn-teaser-headline uk-margin-remove@show-grid\"):\n",
    "    event_details = {}\n",
    "    \n",
    "    # Extract event name from the nested <a> tag\n",
    "    name_tag = event.find('a', href=True)\n",
    "    event_details['name'] = name_tag.text.strip() if name_tag else \"No name available\"\n",
    "    \n",
    "    # Extract event URL from the <a> tag\n",
    "    event_details['url'] = name_tag['href'] if name_tag else \"No URL available\"\n",
    "    \n",
    "    # Extract date, location, and description\n",
    "    date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "    location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "    description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "    event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "    event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "    event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "    \n",
    "    # Append the event details to the list\n",
    "    events.append(event_details)\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('event_data.json', 'w') as json_file:\n",
    "    json.dump(events, json_file, indent=4)\n",
    "\n",
    "print(\"Scraped data saved to event_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06040f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraped data saved to all_event_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Base URL for the event search\n",
    "base_url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch\"\n",
    "\n",
    "# Prepare a list to store the event details from all pages\n",
    "all_events = []\n",
    "\n",
    "# Function to scrape event details from a given page number\n",
    "def scrape_events_from_page(page_num):\n",
    "    url = f\"{base_url}?page={page_num}&v=d\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all event containers\n",
    "    for event in soup.find_all('p', class_=\"fdn-teaser-headline uk-margin-remove@show-grid\"):\n",
    "        event_details = {}\n",
    "        \n",
    "        # Extract event name from the nested <a> tag\n",
    "        name_tag = event.find('a', href=True)\n",
    "        event_details['name'] = name_tag.text.strip() if name_tag else \"No name available\"\n",
    "        \n",
    "        # Extract event URL from the <a> tag\n",
    "        event_details['url'] = name_tag['href'] if name_tag else \"No URL available\"\n",
    "        \n",
    "        # Extract date, location, and description\n",
    "        date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "        location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "        description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "        event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "        event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "        event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "        \n",
    "        # Append the event details to the list of all events\n",
    "        all_events.append(event_details)\n",
    "\n",
    "# Loop through all pages (24 in this case)\n",
    "for page_num in range(1, 25):  # Adjust the range if the number of pages changes\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    scrape_events_from_page(page_num)\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('all_event_data.json', 'w') as json_file:\n",
    "    json.dump(all_events, json_file, indent=4)\n",
    "\n",
    "print(\"Scraped data saved to all_event_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d239a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Page 1 scraped successfully with 30 events.\n",
      "Scraping page 2...\n",
      "Page 2 scraped successfully with 30 events.\n",
      "Scraping page 3...\n",
      "Page 3 scraped successfully with 30 events.\n",
      "Scraping page 4...\n",
      "Page 4 scraped successfully with 30 events.\n",
      "Scraping page 5...\n",
      "Page 5 scraped successfully with 30 events.\n",
      "Scraping page 6...\n",
      "Error while scraping page 6: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Scraping page 7...\n",
      "Error while scraping page 7: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Scraping page 8...\n",
      "Error while scraping page 8: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Scraping page 9...\n",
      "Error while scraping page 9: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Scraping page 10...\n",
      "Error while scraping page 10: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Scraping page 11...\n",
      "Error while scraping page 11: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Scraping page 12...\n",
      "Error while scraping page 12: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Scraping page 13...\n",
      "Error while scraping page 13: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Scraping page 14...\n",
      "Error while scraping page 14: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Scraping page 15...\n",
      "Error while scraping page 15: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Scraping page 16...\n",
      "Error while scraping page 16: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Scraping page 17...\n",
      "Error while scraping page 17: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Scraping page 18...\n",
      "Error while scraping page 18: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Scraping page 19...\n",
      "Error while scraping page 19: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Scraping page 20...\n",
      "Error while scraping page 20: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Scraping page 21...\n",
      "Error while scraping page 21: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Scraping page 22...\n",
      "Error while scraping page 22: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Scraping page 23...\n",
      "Error while scraping page 23: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Scraping page 24...\n",
      "Error while scraping page 24: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=24&v=d\n",
      "Scraped data saved to all_event_data.json with 150 events.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Base URL for the event search\n",
    "base_url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch\"\n",
    "\n",
    "# Prepare a list to store the event details from all pages\n",
    "all_events = []\n",
    "\n",
    "# Function to scrape event details from a given page number\n",
    "def scrape_events_from_page(page_num):\n",
    "    try:\n",
    "        url = f\"{base_url}?page={page_num}&v=d\"\n",
    "        response = requests.get(url, timeout=10)  # Set a timeout for the request\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all event containers\n",
    "        event_count = 0\n",
    "        for event in soup.find_all('p', class_=\"fdn-teaser-headline uk-margin-remove@show-grid\"):\n",
    "            event_details = {}\n",
    "            \n",
    "            # Extract event name from the nested <a> tag\n",
    "            name_tag = event.find('a', href=True)\n",
    "            event_details['name'] = name_tag.text.strip() if name_tag else \"No name available\"\n",
    "            \n",
    "            # Extract event URL from the <a> tag\n",
    "            event_details['url'] = name_tag['href'] if name_tag else \"No URL available\"\n",
    "            \n",
    "            # Extract date, location, and description\n",
    "            date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "            location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "            description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "            event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "            event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "            event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "            \n",
    "            # Append the event details to the list of all events\n",
    "            all_events.append(event_details)\n",
    "            event_count += 1\n",
    "\n",
    "        print(f\"Page {page_num} scraped successfully with {event_count} events.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while scraping page {page_num}: {e}\")\n",
    "\n",
    "# Loop through all pages (24 in this case)\n",
    "for page_num in range(1, 25):  # Adjust the range if the number of pages changes\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    scrape_events_from_page(page_num)\n",
    "    time.sleep(2)  # Adding a delay between requests to avoid overloading the server\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('all_event_data.json', 'w') as json_file:\n",
    "    json.dump(all_events, json_file, indent=4)\n",
    "\n",
    "print(f\"Scraped data saved to all_event_data.json with {len(all_events)} events.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84a30bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Attempt 1 to scrape page 1...\n",
      "Page 1 scraped successfully with 30 events.\n",
      "Scraping page 2...\n",
      "Attempt 1 to scrape page 2...\n",
      "Page 2 scraped successfully with 30 events.\n",
      "Scraping page 3...\n",
      "Attempt 1 to scrape page 3...\n",
      "Page 3 scraped successfully with 30 events.\n",
      "Scraping page 4...\n",
      "Attempt 1 to scrape page 4...\n",
      "Page 4 scraped successfully with 30 events.\n",
      "Scraping page 5...\n",
      "Attempt 1 to scrape page 5...\n",
      "Page 5 scraped successfully with 30 events.\n",
      "Scraping page 6...\n",
      "Attempt 1 to scrape page 6...\n",
      "Error on attempt 1 for page 6: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Attempt 2 to scrape page 6...\n",
      "Error on attempt 2 for page 6: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Attempt 3 to scrape page 6...\n",
      "Error on attempt 3 for page 6: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Failed to scrape page 6 after 3 attempts.\n",
      "Scraping page 7...\n",
      "Attempt 1 to scrape page 7...\n",
      "Error on attempt 1 for page 7: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Attempt 2 to scrape page 7...\n",
      "Error on attempt 2 for page 7: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Attempt 3 to scrape page 7...\n",
      "Error on attempt 3 for page 7: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Failed to scrape page 7 after 3 attempts.\n",
      "Scraping page 8...\n",
      "Attempt 1 to scrape page 8...\n",
      "Error on attempt 1 for page 8: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Attempt 2 to scrape page 8...\n",
      "Error on attempt 2 for page 8: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Attempt 3 to scrape page 8...\n",
      "Error on attempt 3 for page 8: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Failed to scrape page 8 after 3 attempts.\n",
      "Scraping page 9...\n",
      "Attempt 1 to scrape page 9...\n",
      "Error on attempt 1 for page 9: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Attempt 2 to scrape page 9...\n",
      "Error on attempt 2 for page 9: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Attempt 3 to scrape page 9...\n",
      "Error on attempt 3 for page 9: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Failed to scrape page 9 after 3 attempts.\n",
      "Scraping page 10...\n",
      "Attempt 1 to scrape page 10...\n",
      "Error on attempt 1 for page 10: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Attempt 2 to scrape page 10...\n",
      "Error on attempt 2 for page 10: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Attempt 3 to scrape page 10...\n",
      "Error on attempt 3 for page 10: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Failed to scrape page 10 after 3 attempts.\n",
      "Scraping page 11...\n",
      "Attempt 1 to scrape page 11...\n",
      "Error on attempt 1 for page 11: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Attempt 2 to scrape page 11...\n",
      "Error on attempt 2 for page 11: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Attempt 3 to scrape page 11...\n",
      "Error on attempt 3 for page 11: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Failed to scrape page 11 after 3 attempts.\n",
      "Scraping page 12...\n",
      "Attempt 1 to scrape page 12...\n",
      "Error on attempt 1 for page 12: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Attempt 2 to scrape page 12...\n",
      "Error on attempt 2 for page 12: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Attempt 3 to scrape page 12...\n",
      "Error on attempt 3 for page 12: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Failed to scrape page 12 after 3 attempts.\n",
      "Scraping page 13...\n",
      "Attempt 1 to scrape page 13...\n",
      "Error on attempt 1 for page 13: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Attempt 2 to scrape page 13...\n",
      "Error on attempt 2 for page 13: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Attempt 3 to scrape page 13...\n",
      "Error on attempt 3 for page 13: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Failed to scrape page 13 after 3 attempts.\n",
      "Scraping page 14...\n",
      "Attempt 1 to scrape page 14...\n",
      "Error on attempt 1 for page 14: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Attempt 2 to scrape page 14...\n",
      "Error on attempt 2 for page 14: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Attempt 3 to scrape page 14...\n",
      "Error on attempt 3 for page 14: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Failed to scrape page 14 after 3 attempts.\n",
      "Scraping page 15...\n",
      "Attempt 1 to scrape page 15...\n",
      "Error on attempt 1 for page 15: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Attempt 2 to scrape page 15...\n",
      "Error on attempt 2 for page 15: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Attempt 3 to scrape page 15...\n",
      "Error on attempt 3 for page 15: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Failed to scrape page 15 after 3 attempts.\n",
      "Scraping page 16...\n",
      "Attempt 1 to scrape page 16...\n",
      "Error on attempt 1 for page 16: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Attempt 2 to scrape page 16...\n",
      "Error on attempt 2 for page 16: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Attempt 3 to scrape page 16...\n",
      "Error on attempt 3 for page 16: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Failed to scrape page 16 after 3 attempts.\n",
      "Scraping page 17...\n",
      "Attempt 1 to scrape page 17...\n",
      "Error on attempt 1 for page 17: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Attempt 2 to scrape page 17...\n",
      "Error on attempt 2 for page 17: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Attempt 3 to scrape page 17...\n",
      "Error on attempt 3 for page 17: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Failed to scrape page 17 after 3 attempts.\n",
      "Scraping page 18...\n",
      "Attempt 1 to scrape page 18...\n",
      "Error on attempt 1 for page 18: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Attempt 2 to scrape page 18...\n",
      "Error on attempt 2 for page 18: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Attempt 3 to scrape page 18...\n",
      "Error on attempt 3 for page 18: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Failed to scrape page 18 after 3 attempts.\n",
      "Scraping page 19...\n",
      "Attempt 1 to scrape page 19...\n",
      "Error on attempt 1 for page 19: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Attempt 2 to scrape page 19...\n",
      "Error on attempt 2 for page 19: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Attempt 3 to scrape page 19...\n",
      "Error on attempt 3 for page 19: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Failed to scrape page 19 after 3 attempts.\n",
      "Scraping page 20...\n",
      "Attempt 1 to scrape page 20...\n",
      "Error on attempt 1 for page 20: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Attempt 2 to scrape page 20...\n",
      "Error on attempt 2 for page 20: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Attempt 3 to scrape page 20...\n",
      "Error on attempt 3 for page 20: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape page 20 after 3 attempts.\n",
      "Scraping page 21...\n",
      "Attempt 1 to scrape page 21...\n",
      "Error on attempt 1 for page 21: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Attempt 2 to scrape page 21...\n",
      "Error on attempt 2 for page 21: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Attempt 3 to scrape page 21...\n",
      "Error on attempt 3 for page 21: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Failed to scrape page 21 after 3 attempts.\n",
      "Scraping page 22...\n",
      "Attempt 1 to scrape page 22...\n",
      "Error on attempt 1 for page 22: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Attempt 2 to scrape page 22...\n",
      "Error on attempt 2 for page 22: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Attempt 3 to scrape page 22...\n",
      "Error on attempt 3 for page 22: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Failed to scrape page 22 after 3 attempts.\n",
      "Scraping page 23...\n",
      "Attempt 1 to scrape page 23...\n",
      "Error on attempt 1 for page 23: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Attempt 2 to scrape page 23...\n",
      "Error on attempt 2 for page 23: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Attempt 3 to scrape page 23...\n",
      "Error on attempt 3 for page 23: 524 Server Error:  for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Failed to scrape page 23 after 3 attempts.\n",
      "Scraping page 24...\n",
      "Attempt 1 to scrape page 24...\n",
      "Page 24 scraped successfully with 17 events.\n",
      "Scraped data saved to all_event_data.json with 167 events.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Base URL for the event search\n",
    "base_url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch\"\n",
    "\n",
    "# Prepare a list to store the event details from all pages\n",
    "all_events = []\n",
    "\n",
    "# Function to scrape event details from a given page number\n",
    "def scrape_events_from_page(page_num, retries=3):\n",
    "    url = f\"{base_url}?page={page_num}&v=d\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} to scrape page {page_num}...\")\n",
    "            response = requests.get(url, timeout=15)  # Set a timeout for the request\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all event containers\n",
    "            event_count = 0\n",
    "            for event in soup.find_all('p', class_=\"fdn-teaser-headline uk-margin-remove@show-grid\"):\n",
    "                event_details = {}\n",
    "                \n",
    "                # Extract event name from the nested <a> tag\n",
    "                name_tag = event.find('a', href=True)\n",
    "                event_details['name'] = name_tag.text.strip() if name_tag else \"No name available\"\n",
    "                \n",
    "                # Extract event URL from the <a> tag\n",
    "                event_details['url'] = name_tag['href'] if name_tag else \"No URL available\"\n",
    "                \n",
    "                # Extract date, location, and description\n",
    "                date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "                location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "                description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "                event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "                event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "                event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "                \n",
    "                # Append the event details to the list of all events\n",
    "                all_events.append(event_details)\n",
    "                event_count += 1\n",
    "\n",
    "            print(f\"Page {page_num} scraped successfully with {event_count} events.\")\n",
    "            return  # If successful, exit the function\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error on attempt {attempt + 1} for page {page_num}: {e}\")\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "    \n",
    "    # If all attempts fail, log an error and continue to the next page\n",
    "    print(f\"Failed to scrape page {page_num} after {retries} attempts.\")\n",
    "\n",
    "# Loop through all pages (24 in this case)\n",
    "for page_num in range(1, 25):  # Adjust the range if the number of pages changes\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    scrape_events_from_page(page_num)\n",
    "    time.sleep(2)  # Adding a delay between requests to avoid overloading the server\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('all_event_data.json', 'w') as json_file:\n",
    "    json.dump(all_events, json_file, indent=4)\n",
    "\n",
    "print(f\"Scraped data saved to all_event_data.json with {len(all_events)} events.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34fe60ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Attempt 1 to scrape page 1...\n",
      "Page 1 scraped successfully with 30 events.\n",
      "Waiting for 6.18 seconds before scraping the next page...\n",
      "Scraping page 2...\n",
      "Attempt 1 to scrape page 2...\n",
      "Page 2 scraped successfully with 30 events.\n",
      "Waiting for 6.40 seconds before scraping the next page...\n",
      "Scraping page 3...\n",
      "Attempt 1 to scrape page 3...\n",
      "Page 3 scraped successfully with 30 events.\n",
      "Waiting for 7.52 seconds before scraping the next page...\n",
      "Scraping page 4...\n",
      "Attempt 1 to scrape page 4...\n",
      "Page 4 scraped successfully with 30 events.\n",
      "Waiting for 7.01 seconds before scraping the next page...\n",
      "Scraping page 5...\n",
      "Attempt 1 to scrape page 5...\n",
      "Page 5 scraped successfully with 30 events.\n",
      "Waiting for 6.56 seconds before scraping the next page...\n",
      "Scraping page 6...\n",
      "Attempt 1 to scrape page 6...\n",
      "Error on attempt 1 for page 6: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Retrying after 5.67 seconds...\n",
      "Attempt 2 to scrape page 6...\n",
      "Error on attempt 2 for page 6: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Retrying after 11.00 seconds...\n",
      "Attempt 3 to scrape page 6...\n",
      "Error on attempt 3 for page 6: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=6&v=d\n",
      "Retrying after 20.02 seconds...\n",
      "Failed to scrape page 6 after 3 attempts.\n",
      "Waiting for 7.35 seconds before scraping the next page...\n",
      "Scraping page 7...\n",
      "Attempt 1 to scrape page 7...\n",
      "Error on attempt 1 for page 7: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Retrying after 5.19 seconds...\n",
      "Attempt 2 to scrape page 7...\n",
      "Error on attempt 2 for page 7: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Retrying after 10.56 seconds...\n",
      "Attempt 3 to scrape page 7...\n",
      "Error on attempt 3 for page 7: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=7&v=d\n",
      "Retrying after 20.29 seconds...\n",
      "Failed to scrape page 7 after 3 attempts.\n",
      "Waiting for 7.47 seconds before scraping the next page...\n",
      "Scraping page 8...\n",
      "Attempt 1 to scrape page 8...\n",
      "Error on attempt 1 for page 8: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Retrying after 5.90 seconds...\n",
      "Attempt 2 to scrape page 8...\n",
      "Error on attempt 2 for page 8: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Retrying after 10.43 seconds...\n",
      "Attempt 3 to scrape page 8...\n",
      "Error on attempt 3 for page 8: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=8&v=d\n",
      "Retrying after 20.40 seconds...\n",
      "Failed to scrape page 8 after 3 attempts.\n",
      "Waiting for 7.07 seconds before scraping the next page...\n",
      "Scraping page 9...\n",
      "Attempt 1 to scrape page 9...\n",
      "Error on attempt 1 for page 9: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Retrying after 5.28 seconds...\n",
      "Attempt 2 to scrape page 9...\n",
      "Error on attempt 2 for page 9: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Retrying after 10.07 seconds...\n",
      "Attempt 3 to scrape page 9...\n",
      "Error on attempt 3 for page 9: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=9&v=d\n",
      "Retrying after 20.38 seconds...\n",
      "Failed to scrape page 9 after 3 attempts.\n",
      "Waiting for 7.38 seconds before scraping the next page...\n",
      "Scraping page 10...\n",
      "Attempt 1 to scrape page 10...\n",
      "Error on attempt 1 for page 10: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Retrying after 5.10 seconds...\n",
      "Attempt 2 to scrape page 10...\n",
      "Error on attempt 2 for page 10: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Retrying after 10.55 seconds...\n",
      "Attempt 3 to scrape page 10...\n",
      "Error on attempt 3 for page 10: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=10&v=d\n",
      "Retrying after 20.82 seconds...\n",
      "Failed to scrape page 10 after 3 attempts.\n",
      "Waiting for 6.53 seconds before scraping the next page...\n",
      "Scraping page 11...\n",
      "Attempt 1 to scrape page 11...\n",
      "Error on attempt 1 for page 11: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Retrying after 5.31 seconds...\n",
      "Attempt 2 to scrape page 11...\n",
      "Error on attempt 2 for page 11: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Retrying after 10.51 seconds...\n",
      "Attempt 3 to scrape page 11...\n",
      "Error on attempt 3 for page 11: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=11&v=d\n",
      "Retrying after 20.56 seconds...\n",
      "Failed to scrape page 11 after 3 attempts.\n",
      "Waiting for 6.26 seconds before scraping the next page...\n",
      "Scraping page 12...\n",
      "Attempt 1 to scrape page 12...\n",
      "Error on attempt 1 for page 12: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Retrying after 5.03 seconds...\n",
      "Attempt 2 to scrape page 12...\n",
      "Error on attempt 2 for page 12: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Retrying after 10.09 seconds...\n",
      "Attempt 3 to scrape page 12...\n",
      "Error on attempt 3 for page 12: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=12&v=d\n",
      "Retrying after 20.41 seconds...\n",
      "Failed to scrape page 12 after 3 attempts.\n",
      "Waiting for 6.42 seconds before scraping the next page...\n",
      "Scraping page 13...\n",
      "Attempt 1 to scrape page 13...\n",
      "Error on attempt 1 for page 13: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Retrying after 5.94 seconds...\n",
      "Attempt 2 to scrape page 13...\n",
      "Error on attempt 2 for page 13: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Retrying after 10.74 seconds...\n",
      "Attempt 3 to scrape page 13...\n",
      "Error on attempt 3 for page 13: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=13&v=d\n",
      "Retrying after 20.04 seconds...\n",
      "Failed to scrape page 13 after 3 attempts.\n",
      "Waiting for 7.53 seconds before scraping the next page...\n",
      "Scraping page 14...\n",
      "Attempt 1 to scrape page 14...\n",
      "Error on attempt 1 for page 14: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Retrying after 5.01 seconds...\n",
      "Attempt 2 to scrape page 14...\n",
      "Error on attempt 2 for page 14: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Retrying after 10.96 seconds...\n",
      "Attempt 3 to scrape page 14...\n",
      "Error on attempt 3 for page 14: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=14&v=d\n",
      "Retrying after 20.34 seconds...\n",
      "Failed to scrape page 14 after 3 attempts.\n",
      "Waiting for 6.52 seconds before scraping the next page...\n",
      "Scraping page 15...\n",
      "Attempt 1 to scrape page 15...\n",
      "Error on attempt 1 for page 15: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Retrying after 5.73 seconds...\n",
      "Attempt 2 to scrape page 15...\n",
      "Error on attempt 2 for page 15: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Retrying after 10.37 seconds...\n",
      "Attempt 3 to scrape page 15...\n",
      "Error on attempt 3 for page 15: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=15&v=d\n",
      "Retrying after 20.15 seconds...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape page 15 after 3 attempts.\n",
      "Waiting for 6.42 seconds before scraping the next page...\n",
      "Scraping page 16...\n",
      "Attempt 1 to scrape page 16...\n",
      "Error on attempt 1 for page 16: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Retrying after 5.75 seconds...\n",
      "Attempt 2 to scrape page 16...\n",
      "Error on attempt 2 for page 16: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Retrying after 10.06 seconds...\n",
      "Attempt 3 to scrape page 16...\n",
      "Error on attempt 3 for page 16: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=16&v=d\n",
      "Retrying after 20.39 seconds...\n",
      "Failed to scrape page 16 after 3 attempts.\n",
      "Waiting for 7.38 seconds before scraping the next page...\n",
      "Scraping page 17...\n",
      "Attempt 1 to scrape page 17...\n",
      "Error on attempt 1 for page 17: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Retrying after 5.28 seconds...\n",
      "Attempt 2 to scrape page 17...\n",
      "Error on attempt 2 for page 17: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Retrying after 10.15 seconds...\n",
      "Attempt 3 to scrape page 17...\n",
      "Error on attempt 3 for page 17: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=17&v=d\n",
      "Retrying after 20.30 seconds...\n",
      "Failed to scrape page 17 after 3 attempts.\n",
      "Waiting for 6.35 seconds before scraping the next page...\n",
      "Scraping page 18...\n",
      "Attempt 1 to scrape page 18...\n",
      "Error on attempt 1 for page 18: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Retrying after 5.41 seconds...\n",
      "Attempt 2 to scrape page 18...\n",
      "Error on attempt 2 for page 18: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Retrying after 10.82 seconds...\n",
      "Attempt 3 to scrape page 18...\n",
      "Error on attempt 3 for page 18: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=18&v=d\n",
      "Retrying after 20.38 seconds...\n",
      "Failed to scrape page 18 after 3 attempts.\n",
      "Waiting for 7.62 seconds before scraping the next page...\n",
      "Scraping page 19...\n",
      "Attempt 1 to scrape page 19...\n",
      "Error on attempt 1 for page 19: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Retrying after 5.70 seconds...\n",
      "Attempt 2 to scrape page 19...\n",
      "Error on attempt 2 for page 19: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Retrying after 10.12 seconds...\n",
      "Attempt 3 to scrape page 19...\n",
      "Error on attempt 3 for page 19: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=19&v=d\n",
      "Retrying after 20.39 seconds...\n",
      "Failed to scrape page 19 after 3 attempts.\n",
      "Waiting for 7.62 seconds before scraping the next page...\n",
      "Scraping page 20...\n",
      "Attempt 1 to scrape page 20...\n",
      "Error on attempt 1 for page 20: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Retrying after 5.92 seconds...\n",
      "Attempt 2 to scrape page 20...\n",
      "Error on attempt 2 for page 20: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Retrying after 10.12 seconds...\n",
      "Attempt 3 to scrape page 20...\n",
      "Error on attempt 3 for page 20: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=20&v=d\n",
      "Retrying after 20.77 seconds...\n",
      "Failed to scrape page 20 after 3 attempts.\n",
      "Waiting for 7.72 seconds before scraping the next page...\n",
      "Scraping page 21...\n",
      "Attempt 1 to scrape page 21...\n",
      "Error on attempt 1 for page 21: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Retrying after 5.18 seconds...\n",
      "Attempt 2 to scrape page 21...\n",
      "Error on attempt 2 for page 21: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Retrying after 10.27 seconds...\n",
      "Attempt 3 to scrape page 21...\n",
      "Error on attempt 3 for page 21: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=21&v=d\n",
      "Retrying after 20.63 seconds...\n",
      "Failed to scrape page 21 after 3 attempts.\n",
      "Waiting for 6.88 seconds before scraping the next page...\n",
      "Scraping page 22...\n",
      "Attempt 1 to scrape page 22...\n",
      "Error on attempt 1 for page 22: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Retrying after 5.25 seconds...\n",
      "Attempt 2 to scrape page 22...\n",
      "Error on attempt 2 for page 22: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Retrying after 10.56 seconds...\n",
      "Attempt 3 to scrape page 22...\n",
      "Error on attempt 3 for page 22: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=22&v=d\n",
      "Retrying after 20.52 seconds...\n",
      "Failed to scrape page 22 after 3 attempts.\n",
      "Waiting for 6.66 seconds before scraping the next page...\n",
      "Scraping page 23...\n",
      "Attempt 1 to scrape page 23...\n",
      "Error on attempt 1 for page 23: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Retrying after 5.27 seconds...\n",
      "Attempt 2 to scrape page 23...\n",
      "Error on attempt 2 for page 23: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Retrying after 10.11 seconds...\n",
      "Attempt 3 to scrape page 23...\n",
      "Error on attempt 3 for page 23: 550 Server Error: Resource Temporarily Unavailable for url: https://www.pghcitypaper.com/pittsburgh/EventSearch?page=23&v=d\n",
      "Retrying after 20.31 seconds...\n",
      "Failed to scrape page 23 after 3 attempts.\n",
      "Waiting for 7.19 seconds before scraping the next page...\n",
      "Scraping page 24...\n",
      "Attempt 1 to scrape page 24...\n",
      "Page 24 scraped successfully with 17 events.\n",
      "Waiting for 7.96 seconds before scraping the next page...\n",
      "Scraped data saved to all_event_data.json with 167 events.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Base URL for the event search\n",
    "base_url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch\"\n",
    "\n",
    "# Prepare a list to store the event details from all pages\n",
    "all_events = []\n",
    "\n",
    "# Function to scrape event details from a given page number\n",
    "def scrape_events_from_page(page_num, retries=3):\n",
    "    url = f\"{base_url}?page={page_num}&v=d\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} to scrape page {page_num}...\")\n",
    "            response = requests.get(url, timeout=15)  # Set a timeout for the request\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find all event containers\n",
    "            event_count = 0\n",
    "            for event in soup.find_all('p', class_=\"fdn-teaser-headline uk-margin-remove@show-grid\"):\n",
    "                event_details = {}\n",
    "                \n",
    "                # Extract event name from the nested <a> tag\n",
    "                name_tag = event.find('a', href=True)\n",
    "                event_details['name'] = name_tag.text.strip() if name_tag else \"No name available\"\n",
    "                \n",
    "                # Extract event URL from the <a> tag\n",
    "                event_details['url'] = name_tag['href'] if name_tag else \"No URL available\"\n",
    "                \n",
    "                # Extract date, location, and description\n",
    "                date_tag = event.find_next('p', class_=\"fdn-teaser-subheadline\")\n",
    "                location_tag = event.find_next('a', class_=\"fdn-event-teaser-location-link\")\n",
    "                description_tag = event.find_next('div', class_=\"fdn-teaser-description\")\n",
    "\n",
    "                event_details['date'] = date_tag.text.strip() if date_tag else \"No date available\"\n",
    "                event_details['location'] = location_tag.text.strip() if location_tag else \"No location available\"\n",
    "                event_details['description'] = description_tag.text.strip() if description_tag else \"No description available\"\n",
    "                \n",
    "                # Append the event details to the list of all events\n",
    "                all_events.append(event_details)\n",
    "                event_count += 1\n",
    "\n",
    "            print(f\"Page {page_num} scraped successfully with {event_count} events.\")\n",
    "            return  # If successful, exit the function\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            delay = 5 * (2 ** attempt) + random.uniform(0, 1)  # Exponential backoff with some randomness\n",
    "            print(f\"Error on attempt {attempt + 1} for page {page_num}: {e}\")\n",
    "            print(f\"Retrying after {delay:.2f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # If all attempts fail, log an error and continue to the next page\n",
    "    print(f\"Failed to scrape page {page_num} after {retries} attempts.\")\n",
    "\n",
    "# Loop through all pages (24 in this case)\n",
    "for page_num in range(1, 25):  # Adjust the range if the number of pages changes\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    scrape_events_from_page(page_num)\n",
    "    delay_between_pages = 5 + random.uniform(1, 3)  # Add a randomized delay between each page to avoid overloading\n",
    "    print(f\"Waiting for {delay_between_pages:.2f} seconds before scraping the next page...\")\n",
    "    time.sleep(delay_between_pages)\n",
    "\n",
    "# Save the scraped data as a JSON file\n",
    "with open('all_event_data.json', 'w') as json_file:\n",
    "    json.dump(all_events, json_file, indent=4)\n",
    "\n",
    "print(f\"Scraped data saved to all_event_data.json with {len(all_events)} events.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5859e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the events page\n",
    "url = 'https://events.cmu.edu/day/date/20241028'\n",
    "\n",
    "# Sending a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# List to store event details\n",
    "events = []\n",
    "\n",
    "# Find all event containers\n",
    "event_containers = soup.find_all('div', class_='lw_cal_event')\n",
    "\n",
    "# Iterate over each event container to extract details\n",
    "for event in event_containers:\n",
    "    # Extract event ID\n",
    "    event_id = event['data-id']\n",
    "    \n",
    "    # Extract event name (title)\n",
    "    title_tag = event.find('div', class_='lw_events_title').find('a')\n",
    "    event_name = title_tag.text.strip() if title_tag else 'N/A'\n",
    "    \n",
    "    # Extract event summary\n",
    "    summary_tag = event.find('div', class_='lw_events_summary')\n",
    "    summary = summary_tag.text.strip() if summary_tag else 'N/A'\n",
    "    \n",
    "    # Extract event location\n",
    "    location_tag = event.find('div', class_='lw_events_location')\n",
    "    location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "    \n",
    "    # Extract event time\n",
    "    time_tag = event.find('div', class_='lw_events_time')\n",
    "    time = time_tag.text.strip() if time_tag else 'N/A'\n",
    "\n",
    "    # Extract event link\n",
    "    link = title_tag['href'] if title_tag else 'N/A'\n",
    "    \n",
    "    # Add the event details to the list\n",
    "    events.append({\n",
    "        'event_id': event_id,\n",
    "        'event_name': event_name,  # Here is where we label it as event_name\n",
    "        'summary': summary,\n",
    "        'location': location,\n",
    "        'time': time,\n",
    "        'link': link\n",
    "    })\n",
    "\n",
    "# Convert the events list to JSON format\n",
    "events_json = json.dumps(events, indent=4)\n",
    "\n",
    "# Print or save the JSON data\n",
    "print(events_json)\n",
    "\n",
    "# Optionally, save to a JSON file\n",
    "with open('events.json', 'w') as json_file:\n",
    "    json.dump(events, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e98d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Event details have been saved to events.json.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the events page\n",
    "url = 'https://events.cmu.edu/day/date/20241028'\n",
    "\n",
    "# Sending a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# List to store event details\n",
    "events = []\n",
    "\n",
    "# Find all event containers\n",
    "event_containers = soup.find_all('div', class_='lw_cal_event')\n",
    "\n",
    "# Iterate over each event container to extract details\n",
    "for event in event_containers:\n",
    "    # Extract event ID\n",
    "    event_id = event['data-id']\n",
    "    \n",
    "    # Extract event name (title)\n",
    "    title_tag = event.find('div', class_='lw_events_title').find('a')\n",
    "    event_name = title_tag.text.strip() if title_tag else 'N/A'\n",
    "    \n",
    "    # Extract event summary\n",
    "    summary_tag = event.find('div', class_='lw_events_summary')\n",
    "    summary = summary_tag.text.strip() if summary_tag else 'N/A'\n",
    "    \n",
    "    # Extract event location\n",
    "    location_tag = event.find('div', class_='lw_events_location')\n",
    "    location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "    \n",
    "    # Extract event time\n",
    "    time_tag = event.find('div', class_='lw_events_time')\n",
    "    time = time_tag.text.strip() if time_tag else 'N/A'\n",
    "\n",
    "    # Extract event link\n",
    "    link = title_tag['href'] if title_tag else 'N/A'\n",
    "    \n",
    "    # Add the event details to the list\n",
    "    events.append({\n",
    "        'event_id': event_id,\n",
    "        'event_name': event_name,  # Here is where we label it as event_name\n",
    "        'summary': summary,\n",
    "        'location': location,\n",
    "        'time': time,\n",
    "        'link': link\n",
    "    })\n",
    "\n",
    "# Convert the events list to JSON format\n",
    "events_json = json.dumps(events, indent=4)\n",
    "\n",
    "# Print or save the JSON data\n",
    "print(events_json)\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('events.json', 'w') as json_file:\n",
    "    json.dump(events, json_file, indent=4)\n",
    "\n",
    "print(\"Event details have been saved to events.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4a1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page retrieved successfully.\n",
      "No events found. Please check the class name or HTML structure.\n",
      "No events to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL of the events page\n",
    "url = 'https://events.cmu.edu/day/date/20241028'\n",
    "\n",
    "# Sending a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "else:\n",
    "    print(\"Page retrieved successfully.\")\n",
    "\n",
    "# Print the HTML content for inspection (you can comment this out later)\n",
    "# print(response.text)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# List to store event details\n",
    "events = []\n",
    "\n",
    "# Find all event containers\n",
    "event_containers = soup.find_all('div', class_='lw_cal_event')\n",
    "\n",
    "# Check if any event containers were found\n",
    "if not event_containers:\n",
    "    print(\"No events found. Please check the class name or HTML structure.\")\n",
    "else:\n",
    "    # Iterate over each event container to extract details\n",
    "    for event in event_containers:\n",
    "        # Extract event ID\n",
    "        event_id = event['data-id']\n",
    "        \n",
    "        # Extract event name (title)\n",
    "        title_tag = event.find('div', class_='lw_events_title').find('a')\n",
    "        event_name = title_tag.text.strip() if title_tag else 'N/A'\n",
    "        \n",
    "        # Extract event summary\n",
    "        summary_tag = event.find('div', class_='lw_events_summary')\n",
    "        summary = summary_tag.text.strip() if summary_tag else 'N/A'\n",
    "        \n",
    "        # Extract event location\n",
    "        location_tag = event.find('div', class_='lw_events_location')\n",
    "        location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "        \n",
    "        # Extract event time\n",
    "        time_tag = event.find('div', class_='lw_events_time')\n",
    "        time = time_tag.text.strip() if time_tag else 'N/A'\n",
    "\n",
    "        # Extract event link\n",
    "        link = title_tag['href'] if title_tag else 'N/A'\n",
    "        \n",
    "        # Add the event details to the list\n",
    "        events.append({\n",
    "            'event_id': event_id,\n",
    "            'event_name': event_name,\n",
    "            'summary': summary,\n",
    "            'location': location,\n",
    "            'time': time,\n",
    "            'link': link\n",
    "        })\n",
    "\n",
    "# Check if any events were found before writing to JSON\n",
    "if events:\n",
    "    # Convert the events list to JSON format\n",
    "    events_json = json.dumps(events, indent=4)\n",
    "\n",
    "    # Save to a JSON file\n",
    "    with open('events.json', 'w') as json_file:\n",
    "        json.dump(events, json_file, indent=4)\n",
    "\n",
    "    print(\"Event details have been saved to events.json.\")\n",
    "else:\n",
    "    print(\"No events to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5a3a748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found. Please check the class name or HTML structure.\n",
      "No events to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://events.cmu.edu/day/date/20241028\"\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the event list\n",
    "event_list = soup.find(\"div\", class_=\"lw_cal_event_list\")\n",
    "events = []\n",
    "\n",
    "if event_list:\n",
    "    event_items = event_list.find_all(\"div\", class_=\"lw_cal_event\")\n",
    "\n",
    "    for item in event_items:\n",
    "        title_elem = item.find(\"div\", class_=\"lw_events_title\")\n",
    "        location_elem = item.find(\"div\", class_=\"lw_events_location\")\n",
    "        time_elem = item.find(\"div\", class_=\"lw_events_time\")\n",
    "        description_elem = item.find(\"div\", class_=\"lw_events_summary\")\n",
    "\n",
    "        # Extract event details\n",
    "        title = title_elem.get_text(strip=True) if title_elem else \"No Title\"\n",
    "        location = location_elem.get_text(strip=True) if location_elem else \"No Location\"\n",
    "        description = description_elem.get_text(strip=True) if description_elem else \"No Description\"\n",
    "\n",
    "        # Extract start and end time\n",
    "        if time_elem:\n",
    "            time_parts = time_elem.get_text(strip=True).split(\" - \")\n",
    "            start_time = time_parts[0]\n",
    "            end_time = time_parts[1] if len(time_parts) > 1 else \"No End Time\"\n",
    "\n",
    "            # Combine date and time\n",
    "            date_str = \"2024-10-28\"\n",
    "            start_datetime = f\"{date_str} {start_time}\"\n",
    "            end_datetime = f\"{date_str} {end_time}\"\n",
    "        else:\n",
    "            start_datetime = \"No Start Time\"\n",
    "            end_datetime = \"No End Time\"\n",
    "\n",
    "        events.append({\n",
    "            \"title\": title,\n",
    "            \"date\": date_str,\n",
    "            \"start_time\": start_datetime,\n",
    "            \"end_time\": end_datetime,\n",
    "            \"location\": location,\n",
    "            \"description\": description\n",
    "        })\n",
    "else:\n",
    "    print(\"No events found. Please check the class name or HTML structure.\")\n",
    "\n",
    "# Save events to JSON file\n",
    "if events:\n",
    "    with open(\"events.json\", \"w\") as json_file:\n",
    "        json.dump(events, json_file, indent=4)\n",
    "    print(\"Events saved to events.json\")\n",
    "else:\n",
    "    print(\"No events to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c1bcd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events to save.\n",
      "Number of events found: 0\n",
      "Number of events found with explicit wait: 11\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://events.cmu.edu/day/date/20241028\"  # Update the date accordingly\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all event containers\n",
    "event_list = soup.find_all(\"div\", class_=\"lw_cal_event\")\n",
    "events = []\n",
    "\n",
    "for item in event_list:\n",
    "    title_elem = item.find(\"div\", class_=\"lw_events_title\")\n",
    "    location_elem = item.find(\"div\", class_=\"lw_events_location\")\n",
    "    time_elem = item.find(\"div\", class_=\"lw_events_time\")\n",
    "    description_elem = item.find(\"div\", class_=\"lw_events_summary\")\n",
    "\n",
    "    # Extract event details\n",
    "    title = title_elem.get_text(strip=True) if title_elem else \"No Title\"\n",
    "    location = location_elem.get_text(strip=True) if location_elem else \"No Location\"\n",
    "    description = description_elem.get_text(strip=True) if description_elem else \"No Description\"\n",
    "\n",
    "    # Extract start and end time\n",
    "    if time_elem:\n",
    "        time_parts = time_elem.get_text(strip=True).split(\" - \")\n",
    "        start_time = time_parts[0]\n",
    "        end_time = time_parts[1] if len(time_parts) > 1 else \"No End Time\"\n",
    "\n",
    "        # Combine date and time\n",
    "        date_str = \"2024-10-28\"  # Update with the correct date format if necessary\n",
    "        start_datetime = f\"{date_str} {start_time}\"\n",
    "        end_datetime = f\"{date_str} {end_time}\"\n",
    "    else:\n",
    "        start_datetime = \"No Start Time\"\n",
    "        end_datetime = \"No End Time\"\n",
    "\n",
    "    events.append({\n",
    "        \"title\": title,\n",
    "        \"date\": date_str,\n",
    "        \"start_time\": start_datetime,\n",
    "        \"end_time\": end_datetime,\n",
    "        \"location\": location,\n",
    "        \"description\": description\n",
    "    })\n",
    "\n",
    "\n",
    "# Save events to JSON file\n",
    "if events:\n",
    "    with open(\"events.json\", \"w\") as json_file:\n",
    "        json.dump(events, json_file, indent=4)\n",
    "    print(\"Events saved to events.json\")\n",
    "else:\n",
    "    print(\"No events to save.\")\n",
    "    \n",
    "event_list = soup.find_all(\"div\", class_=\"lw_cal_event\")\n",
    "print(\"Number of events found:\", len(event_list))  # Output the number of found events\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up Selenium\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the event elements to be present\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lw_cal_event\")))\n",
    "    event_list = driver.find_elements(By.CLASS_NAME, \"lw_cal_event\")\n",
    "    print(\"Number of events found with explicit wait:\", len(event_list))\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "\n",
    "driver.quit()  # Don't forget to close the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f531c946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events found: 11\n",
      "Event details saved to events.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with the correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://events.cmu.edu/day/date/20241028\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the event elements to be present\n",
    "wait = WebDriverWait(driver, 10)\n",
    "event_list = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lw_cal_event\")))\n",
    "\n",
    "# Extract event details\n",
    "events = []\n",
    "for event in event_list:\n",
    "    # Event name\n",
    "    try:\n",
    "        title_elem = event.find_element(By.CLASS_NAME, \"lw_events_title\")\n",
    "        title = title_elem.text.strip() if title_elem else \"No Title\"\n",
    "    except:\n",
    "        title = \"No Title\"\n",
    "\n",
    "    # Location\n",
    "    try:\n",
    "        location_elem = event.find_element(By.CLASS_NAME, \"lw_events_location\")\n",
    "        location = location_elem.text.strip() if location_elem else \"No Location\"\n",
    "    except:\n",
    "        location = \"No Location\"\n",
    "\n",
    "    # Description\n",
    "    try:\n",
    "        description_elem = event.find_element(By.CLASS_NAME, \"lw_events_summary\")\n",
    "        description = description_elem.text.strip() if description_elem else \"No Description\"\n",
    "    except:\n",
    "        description = \"No Description\"\n",
    "\n",
    "    # Time\n",
    "    try:\n",
    "        time_elem = event.find_element(By.CLASS_NAME, \"lw_events_time\")\n",
    "        time_info = time_elem.text.strip() if time_elem else \"No Time Info\"\n",
    "    except:\n",
    "        time_info = \"No Time Info\"\n",
    "\n",
    "    # Combine date and time\n",
    "    date_str = \"2024-10-28\"\n",
    "    start_datetime = f\"{date_str} {time_info}\"\n",
    "\n",
    "    # Append event data to the list\n",
    "    events.append({\n",
    "        \"event_name\": title,\n",
    "        \"date\": start_datetime,\n",
    "        \"location\": location,\n",
    "        \"description\": description\n",
    "    })\n",
    "\n",
    "# Save events to a JSON file\n",
    "with open(\"events.json\", \"w\") as json_file:\n",
    "    json.dump(events, json_file, indent=4)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Output the number of events\n",
    "print(f\"Number of events found: {len(events)}\")\n",
    "print(\"Event details saved to events.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5551c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped events for 2024-10-08\n",
      "Scraped events for 2024-10-09\n",
      "Scraped events for 2024-10-10\n",
      "Scraped events for 2024-10-11\n",
      "Scraped events for 2024-10-12\n",
      "Scraped events for 2024-10-13\n",
      "Scraped events for 2024-10-14\n",
      "Scraped events for 2024-10-15\n",
      "Scraped events for 2024-10-16\n",
      "Scraped events for 2024-10-17\n",
      "Scraped events for 2024-10-18\n",
      "Scraped events for 2024-10-19\n",
      "Scraped events for 2024-10-20\n",
      "Scraped events for 2024-10-21\n",
      "Scraped events for 2024-10-22\n",
      "Scraped events for 2024-10-23\n",
      "Scraped events for 2024-10-24\n",
      "Scraped events for 2024-10-25\n",
      "Scraped events for 2024-10-26\n",
      "Scraped events for 2024-10-27\n",
      "Scraped events for 2024-10-28\n",
      "Scraped events for 2024-10-29\n",
      "Scraped events for 2024-10-30\n",
      "Scraped events for 2024-10-31\n",
      "Scraped events for 2024-11-01\n",
      "Scraped events for 2024-11-02\n",
      "Scraped events for 2024-11-03\n",
      "Scraped events for 2024-11-04\n",
      "Scraped events for 2024-11-05\n",
      "Scraped events for 2024-11-06\n",
      "Scraped events for 2024-11-07\n",
      "Scraped events for 2024-11-08\n",
      "Scraped events for 2024-11-09\n",
      "Scraped events for 2024-11-10\n",
      "Scraped events for 2024-11-11\n",
      "Scraped events for 2024-11-12\n",
      "Scraped events for 2024-11-13\n",
      "Scraped events for 2024-11-14\n",
      "Scraped events for 2024-11-15\n",
      "Scraped events for 2024-11-16\n",
      "Scraped events for 2024-11-17\n",
      "Scraped events for 2024-11-18\n",
      "Scraped events for 2024-11-19\n",
      "Scraped events for 2024-11-20\n",
      "Scraped events for 2024-11-21\n",
      "Scraped events for 2024-11-22\n",
      "Scraped events for 2024-11-23\n",
      "Scraped events for 2024-11-24\n",
      "Scraped events for 2024-11-25\n",
      "Scraped events for 2024-11-26\n",
      "Scraped events for 2024-11-27\n",
      "Scraped events for 2024-11-28\n",
      "Scraped events for 2024-11-29\n",
      "Scraped events for 2024-11-30\n",
      "Scraped events for 2024-12-01\n",
      "Scraped events for 2024-12-02\n",
      "Scraped events for 2024-12-03\n",
      "Scraped events for 2024-12-04\n",
      "Scraped events for 2024-12-05\n",
      "Scraped events for 2024-12-06\n",
      "Scraped events for 2024-12-07\n",
      "Scraped events for 2024-12-08\n",
      "Scraped events for 2024-12-09\n",
      "Scraped events for 2024-12-10\n",
      "Scraped events for 2024-12-11\n",
      "Scraped events for 2024-12-12\n",
      "Scraped events for 2024-12-13\n",
      "Scraped events for 2024-12-14\n",
      "Scraped events for 2024-12-15\n",
      "Scraped events for 2024-12-16\n",
      "Scraped events for 2024-12-17\n",
      "Scraped events for 2024-12-18\n",
      "Scraped events for 2024-12-19\n",
      "Scraped events for 2024-12-20\n",
      "Scraped events for 2024-12-21\n",
      "Scraped events for 2024-12-22\n",
      "Scraped events for 2024-12-23\n",
      "Scraped events for 2024-12-24\n",
      "Scraped events for 2024-12-25\n",
      "Scraped events for 2024-12-26\n",
      "Scraped events for 2024-12-27\n",
      "Scraped events for 2024-12-28\n",
      "Scraped events for 2024-12-29\n",
      "Scraped events for 2024-12-30\n",
      "Scraped events for 2024-12-31\n",
      "Scraped events for 2025-01-01\n",
      "Scraped events for 2025-01-02\n",
      "Scraped events for 2025-01-03\n",
      "Scraped events for 2025-01-04\n",
      "Scraped events for 2025-01-05\n",
      "Scraped events for 2025-01-06\n",
      "Scraped events for 2025-01-07\n",
      "Scraped events for 2025-01-08\n",
      "Scraped events for 2025-01-09\n",
      "Scraped events for 2025-01-10\n",
      "Scraped events for 2025-01-11\n",
      "Scraped events for 2025-01-12\n",
      "Scraped events for 2025-01-13\n",
      "Scraped events for 2025-01-14\n",
      "Scraped events for 2025-01-15\n",
      "Scraped events for 2025-01-16\n",
      "Scraped events for 2025-01-17\n",
      "Scraped events for 2025-01-18\n",
      "Scraped events for 2025-01-19\n",
      "Scraped events for 2025-01-20\n",
      "Scraped events for 2025-01-21\n",
      "Scraped events for 2025-01-22\n",
      "Scraped events for 2025-01-23\n",
      "Scraped events for 2025-01-24\n",
      "Scraped events for 2025-01-25\n",
      "Scraped events for 2025-01-26\n",
      "Scraped events for 2025-01-27\n",
      "Scraped events for 2025-01-28\n",
      "Scraped events for 2025-01-29\n",
      "Scraped events for 2025-01-30\n",
      "Scraped events for 2025-01-31\n",
      "Scraped events for 2025-02-01\n",
      "Scraped events for 2025-02-02\n",
      "Scraped events for 2025-02-03\n",
      "Scraped events for 2025-02-04\n",
      "Scraped events for 2025-02-05\n",
      "Scraped events for 2025-02-06\n",
      "Scraped events for 2025-02-07\n",
      "Scraped events for 2025-02-08\n",
      "Scraped events for 2025-02-09\n",
      "Scraped events for 2025-02-10\n",
      "Scraped events for 2025-02-11\n",
      "Scraped events for 2025-02-12\n",
      "Scraped events for 2025-02-13\n",
      "Scraped events for 2025-02-14\n",
      "Scraped events for 2025-02-15\n",
      "Scraped events for 2025-02-16\n",
      "Scraped events for 2025-02-17\n",
      "Scraped events for 2025-02-18\n",
      "Scraped events for 2025-02-19\n",
      "Scraped events for 2025-02-20\n",
      "Scraped events for 2025-02-21\n",
      "Scraped events for 2025-02-22\n",
      "Scraped events for 2025-02-23\n",
      "Scraped events for 2025-02-24\n",
      "Scraped events for 2025-02-25\n",
      "Scraped events for 2025-02-26\n",
      "Scraped events for 2025-02-27\n",
      "Scraped events for 2025-02-28\n",
      "Scraped events for 2025-03-01\n",
      "Scraped events for 2025-03-02\n",
      "Scraped events for 2025-03-03\n",
      "Scraped events for 2025-03-04\n",
      "Scraped events for 2025-03-05\n",
      "Scraped events for 2025-03-06\n",
      "Scraped events for 2025-03-07\n",
      "Scraped events for 2025-03-08\n",
      "Scraped events for 2025-03-09\n",
      "Scraped events for 2025-03-10\n",
      "Scraped events for 2025-03-11\n",
      "Scraped events for 2025-03-12\n",
      "Scraped events for 2025-03-13\n",
      "Scraped events for 2025-03-14\n",
      "Scraped events for 2025-03-15\n",
      "Scraped events for 2025-03-16\n",
      "Scraped events for 2025-03-17\n",
      "Scraped events for 2025-03-18\n",
      "Scraped events for 2025-03-19\n",
      "Scraped events for 2025-03-20\n",
      "Scraped events for 2025-03-21\n",
      "Scraped events for 2025-03-22\n",
      "Scraped events for 2025-03-23\n",
      "Scraped events for 2025-03-24\n",
      "Scraped events for 2025-03-25\n",
      "Scraped events for 2025-03-26\n",
      "Scraped events for 2025-03-27\n",
      "Scraped events for 2025-03-28\n",
      "Scraped events for 2025-03-29\n",
      "Scraped events for 2025-03-30\n",
      "Scraped events for 2025-03-31\n",
      "Scraped events for 2025-04-01\n",
      "Scraped events for 2025-04-02\n",
      "Scraped events for 2025-04-03\n",
      "Scraped events for 2025-04-04\n",
      "Scraped events for 2025-04-05\n",
      "Scraped events for 2025-04-06\n",
      "Scraped events for 2025-04-07\n",
      "Scraped events for 2025-04-08\n",
      "Scraped events for 2025-04-09\n",
      "Scraped events for 2025-04-10\n",
      "Scraped events for 2025-04-11\n",
      "Scraped events for 2025-04-12\n",
      "Scraped events for 2025-04-13\n",
      "Scraped events for 2025-04-14\n",
      "Scraped events for 2025-04-15\n",
      "Scraped events for 2025-04-16\n",
      "Scraped events for 2025-04-17\n",
      "Scraped events for 2025-04-18\n",
      "Scraped events for 2025-04-19\n",
      "Scraped events for 2025-04-20\n",
      "Scraped events for 2025-04-21\n",
      "Scraped events for 2025-04-22\n",
      "Scraped events for 2025-04-23\n",
      "Scraped events for 2025-04-24\n",
      "Scraped events for 2025-04-25\n",
      "Scraped events for 2025-04-26\n",
      "Scraped events for 2025-04-27\n",
      "Scraped events for 2025-04-28\n",
      "Scraped events for 2025-04-29\n",
      "Scraped events for 2025-04-30\n",
      "Scraped events for 2025-05-01\n",
      "Scraped events for 2025-05-02\n",
      "Scraped events for 2025-05-03\n",
      "Scraped events for 2025-05-04\n",
      "Scraped events for 2025-05-05\n",
      "Scraped events for 2025-05-06\n",
      "Scraped events for 2025-05-07\n",
      "Scraped events for 2025-05-08\n",
      "Scraped events for 2025-05-09\n",
      "Scraped events for 2025-05-10\n",
      "Scraped events for 2025-05-11\n",
      "Scraped events for 2025-05-12\n",
      "Scraped events for 2025-05-13\n",
      "Scraped events for 2025-05-14\n",
      "Scraped events for 2025-05-15\n",
      "Scraped events for 2025-05-16\n",
      "Scraped events for 2025-05-17\n",
      "Scraped events for 2025-05-18\n",
      "Scraped events for 2025-05-19\n",
      "Scraped events for 2025-05-20\n",
      "Scraped events for 2025-05-21\n",
      "Scraped events for 2025-05-22\n",
      "Scraped events for 2025-05-23\n",
      "Scraped events for 2025-05-24\n",
      "Scraped events for 2025-05-25\n",
      "Scraped events for 2025-05-26\n",
      "Scraped events for 2025-05-27\n",
      "Scraped events for 2025-05-28\n",
      "Scraped events for 2025-05-29\n",
      "Scraped events for 2025-05-30\n",
      "Scraped events for 2025-05-31\n",
      "Scraped events for 2025-06-01\n",
      "No events found or error for 2025-06-02: Message: \n",
      "\n",
      "No events found or error for 2025-06-03: Message: \n",
      "\n",
      "No events found or error for 2025-06-04: Message: \n",
      "\n",
      "No events found or error for 2025-06-05: Message: \n",
      "\n",
      "No events found or error for 2025-06-06: Message: \n",
      "\n",
      "No events found or error for 2025-06-07: Message: \n",
      "\n",
      "No events found or error for 2025-06-08: Message: \n",
      "\n",
      "Scraped events for 2025-06-09\n",
      "No events found or error for 2025-06-10: Message: \n",
      "\n",
      "No events found or error for 2025-06-11: Message: \n",
      "\n",
      "No events found or error for 2025-06-12: Message: \n",
      "\n",
      "Total events scraped: 952\n",
      "All event details saved to all_events.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with your correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Today's date\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Number of days to scrape (365 days for one year)\n",
    "num_days = 248\n",
    "\n",
    "# Initialize an empty list to hold event data for all dates\n",
    "all_event_data = []\n",
    "\n",
    "# Loop through each day for the next year\n",
    "for day_offset in range(num_days):\n",
    "    current_date = start_date + timedelta(days=day_offset)\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")  # Format date as YYYYMMDD for the URL\n",
    "\n",
    "    # URL for the current day\n",
    "    url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the event elements to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        event_list = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lw_cal_event\")))\n",
    "\n",
    "        # Extract event details for the current date\n",
    "        for event in event_list:\n",
    "            # Event name\n",
    "            try:\n",
    "                title_elem = event.find_element(By.CLASS_NAME, \"lw_events_title\")\n",
    "                title = title_elem.text.strip() if title_elem else \"No Title\"\n",
    "            except:\n",
    "                title = \"No Title\"\n",
    "\n",
    "            # Location\n",
    "            try:\n",
    "                location_elem = event.find_element(By.CLASS_NAME, \"lw_events_location\")\n",
    "                location = location_elem.text.strip() if location_elem else \"No Location\"\n",
    "            except:\n",
    "                location = \"No Location\"\n",
    "\n",
    "            # Description\n",
    "            try:\n",
    "                description_elem = event.find_element(By.CLASS_NAME, \"lw_events_summary\")\n",
    "                description = description_elem.text.strip() if description_elem else \"No Description\"\n",
    "            except:\n",
    "                description = \"No Description\"\n",
    "\n",
    "            # Time\n",
    "            try:\n",
    "                time_elem = event.find_element(By.CLASS_NAME, \"lw_events_time\")\n",
    "                time_info = time_elem.text.strip() if time_elem else \"No Time Info\"\n",
    "            except:\n",
    "                time_info = \"No Time Info\"\n",
    "\n",
    "            # Combine date and time\n",
    "            start_datetime = f\"{current_date.strftime('%Y-%m-%d')} {time_info}\"\n",
    "\n",
    "            # Append event data to the list\n",
    "            all_event_data.append({\n",
    "                \"event_name\": title,\n",
    "                \"date\": start_datetime,\n",
    "                \"location\": location,\n",
    "                \"description\": description\n",
    "            })\n",
    "\n",
    "        print(f\"Scraped events for {current_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No events found or error for {current_date.strftime('%Y-%m-%d')}: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver after scraping all days\n",
    "driver.quit()\n",
    "\n",
    "# Save all event data to a JSON file\n",
    "with open(\"all_events.json\", \"w\") as json_file:\n",
    "    json.dump(all_event_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Total events scraped: {len(all_event_data)}\")\n",
    "print(\"All event details saved to all_events.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "724411aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped events for 2024-10-08\n",
      "Scraped events for 2024-10-09\n",
      "Scraped events for 2024-10-10\n",
      "Total events scraped: 48\n",
      "All event details saved to all_events.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with your correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Today's date\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Number of days to scrape (365 days for one year)\n",
    "num_days = 3\n",
    "\n",
    "# Initialize an empty list to hold event data for all dates\n",
    "all_event_data = []\n",
    "\n",
    "# Loop through each day for the next year\n",
    "for day_offset in range(num_days):\n",
    "    current_date = start_date + timedelta(days=day_offset)\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")  # Format date as YYYYMMDD for the URL\n",
    "\n",
    "    # URL for the current day\n",
    "    url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the event elements to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        event_list = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lw_cal_event\")))\n",
    "\n",
    "        # Extract event details for the current date\n",
    "        for event in event_list:\n",
    "            # Event name and event page URL\n",
    "            try:\n",
    "                title_elem = event.find_element(By.CLASS_NAME, \"lw_events_title\").find_element(By.TAG_NAME, \"a\")\n",
    "                title = title_elem.text.strip() if title_elem else \"No Title\"\n",
    "                event_url = title_elem.get_attribute(\"href\")  # Get event page URL\n",
    "            except:\n",
    "                title = \"No Title\"\n",
    "                event_url = None\n",
    "\n",
    "            # Location\n",
    "            try:\n",
    "                location_elem = event.find_element(By.CLASS_NAME, \"lw_events_location\")\n",
    "                location = location_elem.text.strip() if location_elem else \"No Location\"\n",
    "            except:\n",
    "                location = \"No Location\"\n",
    "\n",
    "            # Time\n",
    "            try:\n",
    "                time_elem = event.find_element(By.CLASS_NAME, \"lw_events_time\")\n",
    "                time_info = time_elem.text.strip() if time_elem else \"No Time Info\"\n",
    "            except:\n",
    "                time_info = \"No Time Info\"\n",
    "\n",
    "            # Combine date and time\n",
    "            start_datetime = f\"{current_date.strftime('%Y-%m-%d')} {time_info}\"\n",
    "\n",
    "            # Navigate to the event page to get the full description\n",
    "            if event_url:\n",
    "                driver.get(event_url)\n",
    "                try:\n",
    "                    # Wait for the description element to load\n",
    "                    description_elem = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"lw_calendar_event_description\"))\n",
    "                    )\n",
    "                    description = description_elem.text.strip() if description_elem else \"No Description\"\n",
    "                except Exception as e:\n",
    "                    description = \"No Description\"\n",
    "                finally:\n",
    "                    driver.back()  # Go back to the event listing page\n",
    "\n",
    "            else:\n",
    "                description = \"No Event Page\"\n",
    "\n",
    "            # Append event data to the list\n",
    "            all_event_data.append({\n",
    "                \"event_name\": title,\n",
    "                \"date\": start_datetime,\n",
    "                \"location\": location,\n",
    "                \"description\": description\n",
    "            })\n",
    "\n",
    "        print(f\"Scraped events for {current_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No events found or error for {current_date.strftime('%Y-%m-%d')}: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver after scraping all days\n",
    "driver.quit()\n",
    "\n",
    "# Save all event data to a JSON file\n",
    "with open(\"all_events.json\", \"w\") as json_file:\n",
    "    json.dump(all_event_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Total events scraped: {len(all_event_data)}\")\n",
    "print(\"All event details saved to all_events.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1ea5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped events for 2024-10-08\n",
      "Scraped events for 2024-10-09\n",
      "Scraped events for 2024-10-10\n",
      "Scraped events for 2024-10-11\n",
      "Scraped events for 2024-10-12\n",
      "Scraped events for 2024-10-13\n",
      "Scraped events for 2024-10-14\n",
      "Scraped events for 2024-10-15\n",
      "Scraped events for 2024-10-16\n",
      "Scraped events for 2024-10-17\n",
      "Scraped events for 2024-10-18\n",
      "Scraped events for 2024-10-19\n",
      "Scraped events for 2024-10-20\n",
      "Scraped events for 2024-10-21\n",
      "Scraped events for 2024-10-22\n",
      "Scraped events for 2024-10-23\n",
      "Scraped events for 2024-10-24\n",
      "Scraped events for 2024-10-25\n",
      "Scraped events for 2024-10-26\n",
      "Scraped events for 2024-10-27\n",
      "Scraped events for 2024-10-28\n",
      "Scraped events for 2024-10-29\n",
      "Scraped events for 2024-10-30\n",
      "Scraped events for 2024-10-31\n",
      "Scraped events for 2024-11-01\n",
      "Scraped events for 2024-11-02\n",
      "Scraped events for 2024-11-03\n",
      "Scraped events for 2024-11-04\n",
      "Scraped events for 2024-11-05\n",
      "Scraped events for 2024-11-06\n",
      "Scraped events for 2024-11-07\n",
      "Scraped events for 2024-11-08\n",
      "Scraped events for 2024-11-09\n",
      "Scraped events for 2024-11-10\n",
      "Scraped events for 2024-11-11\n",
      "Scraped events for 2024-11-12\n",
      "Scraped events for 2024-11-13\n",
      "Scraped events for 2024-11-14\n",
      "Scraped events for 2024-11-15\n",
      "Scraped events for 2024-11-16\n",
      "Scraped events for 2024-11-17\n",
      "Scraped events for 2024-11-18\n",
      "Scraped events for 2024-11-19\n",
      "Scraped events for 2024-11-20\n",
      "Scraped events for 2024-11-21\n",
      "Scraped events for 2024-11-22\n",
      "Scraped events for 2024-11-23\n",
      "Scraped events for 2024-11-24\n",
      "Scraped events for 2024-11-25\n",
      "Scraped events for 2024-11-26\n",
      "Scraped events for 2024-11-27\n",
      "Scraped events for 2024-11-28\n",
      "Scraped events for 2024-11-29\n",
      "Scraped events for 2024-11-30\n",
      "Scraped events for 2024-12-01\n",
      "Scraped events for 2024-12-02\n",
      "Scraped events for 2024-12-03\n",
      "Scraped events for 2024-12-04\n",
      "Scraped events for 2024-12-05\n",
      "Scraped events for 2024-12-06\n",
      "Scraped events for 2024-12-07\n",
      "Scraped events for 2024-12-08\n",
      "Scraped events for 2024-12-09\n",
      "Scraped events for 2024-12-10\n",
      "Scraped events for 2024-12-11\n",
      "Scraped events for 2024-12-12\n",
      "Scraped events for 2024-12-13\n",
      "Scraped events for 2024-12-14\n",
      "Scraped events for 2024-12-15\n",
      "Scraped events for 2024-12-16\n",
      "Scraped events for 2024-12-17\n",
      "Scraped events for 2024-12-18\n",
      "Scraped events for 2024-12-19\n",
      "Scraped events for 2024-12-20\n",
      "Scraped events for 2024-12-21\n",
      "Scraped events for 2024-12-22\n",
      "Scraped events for 2024-12-23\n",
      "Scraped events for 2024-12-24\n",
      "Scraped events for 2024-12-25\n",
      "Scraped events for 2024-12-26\n",
      "Scraped events for 2024-12-27\n",
      "Scraped events for 2024-12-28\n",
      "Scraped events for 2024-12-29\n",
      "Scraped events for 2024-12-30\n",
      "Scraped events for 2024-12-31\n",
      "Scraped events for 2025-01-01\n",
      "Scraped events for 2025-01-02\n",
      "Scraped events for 2025-01-03\n",
      "Scraped events for 2025-01-04\n",
      "Scraped events for 2025-01-05\n",
      "Scraped events for 2025-01-06\n",
      "Scraped events for 2025-01-07\n",
      "Scraped events for 2025-01-08\n",
      "Scraped events for 2025-01-09\n",
      "Scraped events for 2025-01-10\n",
      "Scraped events for 2025-01-11\n",
      "Scraped events for 2025-01-12\n",
      "Scraped events for 2025-01-13\n",
      "Scraped events for 2025-01-14\n",
      "Scraped events for 2025-01-15\n",
      "Scraped events for 2025-01-16\n",
      "Scraped events for 2025-01-17\n",
      "Scraped events for 2025-01-18\n",
      "Scraped events for 2025-01-19\n",
      "Scraped events for 2025-01-20\n",
      "Scraped events for 2025-01-21\n",
      "Scraped events for 2025-01-22\n",
      "Scraped events for 2025-01-23\n",
      "Scraped events for 2025-01-24\n",
      "Scraped events for 2025-01-25\n",
      "Scraped events for 2025-01-26\n",
      "Scraped events for 2025-01-27\n",
      "Scraped events for 2025-01-28\n",
      "Scraped events for 2025-01-29\n",
      "Scraped events for 2025-01-30\n",
      "Scraped events for 2025-01-31\n",
      "Scraped events for 2025-02-01\n",
      "Scraped events for 2025-02-02\n",
      "Scraped events for 2025-02-03\n",
      "Scraped events for 2025-02-04\n",
      "Scraped events for 2025-02-05\n",
      "Scraped events for 2025-02-06\n",
      "Scraped events for 2025-02-07\n",
      "Scraped events for 2025-02-08\n",
      "Scraped events for 2025-02-09\n",
      "Scraped events for 2025-02-10\n",
      "Scraped events for 2025-02-11\n",
      "Scraped events for 2025-02-12\n",
      "Scraped events for 2025-02-13\n",
      "Scraped events for 2025-02-14\n",
      "Scraped events for 2025-02-15\n",
      "Scraped events for 2025-02-16\n",
      "Scraped events for 2025-02-17\n",
      "Scraped events for 2025-02-18\n",
      "Scraped events for 2025-02-19\n",
      "Scraped events for 2025-02-20\n",
      "Scraped events for 2025-02-21\n",
      "Scraped events for 2025-02-22\n",
      "Scraped events for 2025-02-23\n",
      "Scraped events for 2025-02-24\n",
      "Scraped events for 2025-02-25\n",
      "Scraped events for 2025-02-26\n",
      "Scraped events for 2025-02-27\n",
      "Scraped events for 2025-02-28\n",
      "Scraped events for 2025-03-01\n",
      "Scraped events for 2025-03-02\n",
      "Scraped events for 2025-03-03\n",
      "Scraped events for 2025-03-04\n",
      "Scraped events for 2025-03-05\n",
      "Scraped events for 2025-03-06\n",
      "Scraped events for 2025-03-07\n",
      "Scraped events for 2025-03-08\n",
      "Scraped events for 2025-03-09\n",
      "Scraped events for 2025-03-10\n",
      "Scraped events for 2025-03-11\n",
      "Scraped events for 2025-03-12\n",
      "Scraped events for 2025-03-13\n",
      "Scraped events for 2025-03-14\n",
      "Scraped events for 2025-03-15\n",
      "Scraped events for 2025-03-16\n",
      "Scraped events for 2025-03-17\n",
      "Scraped events for 2025-03-18\n",
      "Scraped events for 2025-03-19\n",
      "Scraped events for 2025-03-20\n",
      "Scraped events for 2025-03-21\n",
      "Scraped events for 2025-03-22\n",
      "Scraped events for 2025-03-23\n",
      "Scraped events for 2025-03-24\n",
      "Scraped events for 2025-03-25\n",
      "Scraped events for 2025-03-26\n",
      "Scraped events for 2025-03-27\n",
      "Scraped events for 2025-03-28\n",
      "Scraped events for 2025-03-29\n",
      "Scraped events for 2025-03-30\n",
      "Scraped events for 2025-03-31\n",
      "Scraped events for 2025-04-01\n",
      "Scraped events for 2025-04-02\n",
      "Scraped events for 2025-04-03\n",
      "Scraped events for 2025-04-04\n",
      "Scraped events for 2025-04-05\n",
      "Scraped events for 2025-04-06\n",
      "Scraped events for 2025-04-07\n",
      "Scraped events for 2025-04-08\n",
      "Scraped events for 2025-04-09\n",
      "Scraped events for 2025-04-10\n",
      "Scraped events for 2025-04-11\n",
      "Scraped events for 2025-04-12\n",
      "Scraped events for 2025-04-13\n",
      "Scraped events for 2025-04-14\n",
      "Scraped events for 2025-04-15\n",
      "Scraped events for 2025-04-16\n",
      "Scraped events for 2025-04-17\n",
      "Scraped events for 2025-04-18\n",
      "Scraped events for 2025-04-19\n",
      "Scraped events for 2025-04-20\n",
      "Scraped events for 2025-04-21\n",
      "Scraped events for 2025-04-22\n",
      "Scraped events for 2025-04-23\n",
      "Scraped events for 2025-04-24\n",
      "Scraped events for 2025-04-25\n",
      "Scraped events for 2025-04-26\n",
      "Scraped events for 2025-04-27\n",
      "Scraped events for 2025-04-28\n",
      "Scraped events for 2025-04-29\n",
      "Scraped events for 2025-04-30\n",
      "Scraped events for 2025-05-01\n",
      "Scraped events for 2025-05-02\n",
      "Scraped events for 2025-05-03\n",
      "Scraped events for 2025-05-04\n",
      "Scraped events for 2025-05-05\n",
      "Scraped events for 2025-05-06\n",
      "Scraped events for 2025-05-07\n",
      "Scraped events for 2025-05-08\n",
      "Scraped events for 2025-05-09\n",
      "Scraped events for 2025-05-10\n",
      "Scraped events for 2025-05-11\n",
      "Scraped events for 2025-05-12\n",
      "Scraped events for 2025-05-13\n",
      "Scraped events for 2025-05-14\n",
      "Scraped events for 2025-05-15\n",
      "Scraped events for 2025-05-16\n",
      "Scraped events for 2025-05-17\n",
      "Scraped events for 2025-05-18\n",
      "Scraped events for 2025-05-19\n",
      "Scraped events for 2025-05-20\n",
      "Scraped events for 2025-05-21\n",
      "Scraped events for 2025-05-22\n",
      "Scraped events for 2025-05-23\n",
      "Scraped events for 2025-05-24\n",
      "Scraped events for 2025-05-25\n",
      "Scraped events for 2025-05-26\n",
      "Scraped events for 2025-05-27\n",
      "Scraped events for 2025-05-28\n",
      "Scraped events for 2025-05-29\n",
      "Scraped events for 2025-05-30\n",
      "Scraped events for 2025-05-31\n",
      "Scraped events for 2025-06-01\n",
      "No events found or error for 2025-06-02: Message: \n",
      "\n",
      "No events found or error for 2025-06-03: Message: \n",
      "\n",
      "No events found or error for 2025-06-04: Message: \n",
      "\n",
      "No events found or error for 2025-06-05: Message: \n",
      "\n",
      "No events found or error for 2025-06-06: Message: \n",
      "\n",
      "No events found or error for 2025-06-07: Message: \n",
      "\n",
      "No events found or error for 2025-06-08: Message: \n",
      "\n",
      "Scraped events for 2025-06-09\n",
      "No events found or error for 2025-06-10: Message: \n",
      "\n",
      "No events found or error for 2025-06-11: Message: \n",
      "\n",
      "No events found or error for 2025-06-12: Message: \n",
      "\n",
      "Total events scraped: 957\n",
      "All event details saved to all_events_with_full_location.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with your correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Today's date\n",
    "start_date = datetime.now()\n",
    "\n",
    "# Number of days to scrape (365 days for one year)\n",
    "num_days = 248\n",
    "\n",
    "# Initialize an empty list to hold event data for all dates\n",
    "all_event_data = []\n",
    "\n",
    "# Loop through each day for the next year\n",
    "for day_offset in range(num_days):\n",
    "    current_date = start_date + timedelta(days=day_offset)\n",
    "    date_str = current_date.strftime(\"%Y%m%d\")  # Format date as YYYYMMDD for the URL\n",
    "\n",
    "    # URL for the current day\n",
    "    url = f\"https://events.cmu.edu/day/date/{date_str}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the event elements to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        event_list = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lw_cal_event\")))\n",
    "\n",
    "        # Extract event details for the current date\n",
    "        for event in event_list:\n",
    "            # Event name and event page URL\n",
    "            try:\n",
    "                title_elem = event.find_element(By.CLASS_NAME, \"lw_events_title\").find_element(By.TAG_NAME, \"a\")\n",
    "                title = title_elem.text.strip() if title_elem else \"No Title\"\n",
    "                event_url = title_elem.get_attribute(\"href\")  # Get event page URL\n",
    "            except:\n",
    "                title = \"No Title\"\n",
    "                event_url = None\n",
    "\n",
    "            # Location from the event list page (fallback)\n",
    "            try:\n",
    "                location_elem = event.find_element(By.CLASS_NAME, \"lw_events_location\")\n",
    "                location = location_elem.text.strip() if location_elem else \"No Location\"\n",
    "            except:\n",
    "                location = \"No Location\"\n",
    "\n",
    "            # Time\n",
    "            try:\n",
    "                time_elem = event.find_element(By.CLASS_NAME, \"lw_events_time\")\n",
    "                time_info = time_elem.text.strip() if time_elem else \"No Time Info\"\n",
    "            except:\n",
    "                time_info = \"No Time Info\"\n",
    "\n",
    "            # Combine date and time\n",
    "            start_datetime = f\"{current_date.strftime('%Y-%m-%d')} {time_info}\"\n",
    "\n",
    "            # Navigate to the event page to get the full description and full location\n",
    "            if event_url:\n",
    "                driver.get(event_url)\n",
    "                try:\n",
    "                    # Wait for the description element to load\n",
    "                    description_elem = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"lw_calendar_event_description\"))\n",
    "                    )\n",
    "                    description = description_elem.text.strip() if description_elem else \"No Description\"\n",
    "                    \n",
    "                    # Get the full location from the event page\n",
    "                    location_elem_full = driver.find_element(By.CLASS_NAME, \"lw_start_time\").find_element(By.XPATH, \"..\")  # Get parent <p> tag containing location details\n",
    "                    full_location = location_elem_full.text.strip() if location_elem_full else location  # If no full location, fallback to earlier location\n",
    "\n",
    "                except Exception as e:\n",
    "                    description = \"No Description\"\n",
    "                    full_location = location  # Fallback in case of failure to fetch full location\n",
    "\n",
    "                finally:\n",
    "                    driver.back()  # Go back to the event listing page\n",
    "\n",
    "            else:\n",
    "                description = \"No Event Page\"\n",
    "                full_location = location  # Fallback\n",
    "\n",
    "            # Append event data to the list\n",
    "            all_event_data.append({\n",
    "                \"event_name\": title,\n",
    "                \"date\": start_datetime,\n",
    "                \"location\": full_location,  # Now saving full location\n",
    "                \"description\": description\n",
    "            })\n",
    "\n",
    "        print(f\"Scraped events for {current_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No events found or error for {current_date.strftime('%Y-%m-%d')}: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver after scraping all days\n",
    "driver.quit()\n",
    "\n",
    "# Save all event data to a JSON file\n",
    "with open(\"all_events_with_full_location.json\", \"w\") as json_file:\n",
    "    json.dump(all_event_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Total events scraped: {len(all_event_data)}\")\n",
    "print(\"All event details saved to all_events_with_full_location.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9ffddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 events successfully!\n",
      "All event details saved to downtown_pittsburgh_events.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with your correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# URL for Downtown Pittsburgh Events\n",
    "url = \"https://downtownpittsburgh.com/events/?n=10&d=28&y=2024\"\n",
    "driver.get(url)\n",
    "\n",
    "# Initialize an empty list to hold event data\n",
    "event_data = []\n",
    "\n",
    "try:\n",
    "    # Wait for the event elements to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    event_list = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"eventitem\")))\n",
    "\n",
    "    # Loop through each event on the main page\n",
    "    for event in event_list:\n",
    "        # Extract Event Name\n",
    "        try:\n",
    "            event_name_elem = event.find_element(By.TAG_NAME, \"h1\").find_element(By.TAG_NAME, \"a\")\n",
    "            event_name = event_name_elem.text.strip() if event_name_elem else \"No Event Name\"\n",
    "            event_url = event_name_elem.get_attribute(\"href\")  # Get event page URL\n",
    "        except:\n",
    "            event_name = \"No Event Name\"\n",
    "            event_url = None\n",
    "\n",
    "        # Extract Date/Time\n",
    "        try:\n",
    "            date_elem = event.find_element(By.CLASS_NAME, \"eventdate\")\n",
    "            event_date = date_elem.text.strip() if date_elem else \"No Date\"\n",
    "        except:\n",
    "            event_date = \"No Date\"\n",
    "\n",
    "        # Extract Description\n",
    "        try:\n",
    "            description_elem = event.find_element(By.CLASS_NAME, \"copyContent\")\n",
    "            event_description = description_elem.text.strip().split(\"READ MORE\")[0]  # Get description up to \"READ MORE\"\n",
    "        except:\n",
    "            event_description = \"No Description\"\n",
    "\n",
    "        # Navigate to individual event page for location\n",
    "        if event_url:\n",
    "            driver.get(event_url)\n",
    "            try:\n",
    "                # Wait for the location element to load\n",
    "                location_elem = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"eventlocation\"))\n",
    "                )\n",
    "                location = location_elem.text.strip() if location_elem else \"No Location\"\n",
    "            except:\n",
    "                location = \"No Location\"\n",
    "            finally:\n",
    "                driver.back()  # Go back to the main event listing page\n",
    "        else:\n",
    "            location = \"No Location\"\n",
    "\n",
    "        # Append event data to the list\n",
    "        event_data.append({\n",
    "            \"event_name\": event_name,\n",
    "            \"date\": event_date,\n",
    "            \"description\": event_description,\n",
    "            \"location\": location\n",
    "        })\n",
    "\n",
    "    print(f\"Scraped {len(event_data)} events successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {str(e)}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Save all event data to a JSON file\n",
    "with open(\"downtown_pittsburgh_events.json\", \"w\") as json_file:\n",
    "    json.dump(event_data, json_file, indent=4)\n",
    "\n",
    "print(\"All event details saved to downtown_pittsburgh_events.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "283f8b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/1qw339l937x_y3n17np39c5c0000gn/T/ipykernel_66279/646753030.py:33: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  description_tag = event.find(text=True, recursive=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Event data has been saved to 'pittsburgh_events.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://downtownpittsburgh.com/events/?n=10&d=28&y=2024'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all event items within the relevant div\n",
    "event_items = soup.select('div.eventitem')\n",
    "\n",
    "# Initialize list to store event data\n",
    "events_data = []\n",
    "\n",
    "# Iterate over each event item\n",
    "for event in event_items:\n",
    "    # Extract event name\n",
    "    event_name = event.find('h1')\n",
    "    if event_name:\n",
    "        event_name = event_name.get_text(strip=True)\n",
    "    else:\n",
    "        event_name = \"No Event Name\"\n",
    "    \n",
    "    # Extract event date\n",
    "    event_date = event.find('div', class_='eventdate')\n",
    "    if event_date:\n",
    "        event_date = event_date.get_text(strip=True)\n",
    "    else:\n",
    "        event_date = \"No Date\"\n",
    "    \n",
    "    # Extract event description\n",
    "    description_tag = event.find(text=True, recursive=False)\n",
    "    if description_tag:\n",
    "        description = description_tag.strip()\n",
    "    else:\n",
    "        description = \"No Description\"\n",
    "    \n",
    "    # Extract link for the event page for further details (location)\n",
    "    more_info_link = event.find('a', class_='button green right')\n",
    "    if more_info_link and more_info_link['href']:\n",
    "        event_page_url = 'https://downtownpittsburgh.com' + more_info_link['href']\n",
    "        \n",
    "        # Fetch event page\n",
    "        event_page_response = requests.get(event_page_url)\n",
    "        event_page_soup = BeautifulSoup(event_page_response.content, 'html.parser')\n",
    "        \n",
    "        # Extract location from the event page\n",
    "        location_tag = event_page_soup.find('div', class_='eventlocation')\n",
    "        if location_tag:\n",
    "            location = location_tag.get_text(separator=\", \", strip=True)\n",
    "        else:\n",
    "            location = \"No Location\"\n",
    "    else:\n",
    "        location = \"No Location\"\n",
    "    \n",
    "    # Store the event details in a dictionary\n",
    "    event_data = {\n",
    "        \"event_name\": event_name,\n",
    "        \"date\": event_date,\n",
    "        \"description\": description,\n",
    "        \"location\": location\n",
    "    }\n",
    "    \n",
    "    # Append to the list of events\n",
    "    events_data.append(event_data)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('pittsburgh_events.json', 'w') as json_file:\n",
    "    json.dump(events_data, json_file, indent=4)\n",
    "\n",
    "print(\"Scraping completed. Event data has been saved to 'pittsburgh_events.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd4fcbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Event data has been saved to 'downtown_pittsburgh_events.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "# Set up the ChromeDriver path\n",
    "service = Service('/usr/local/bin/chromedriver')  # Update with your correct path\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://downtownpittsburgh.com/events/?n=10&d=28&y=2024'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and ensure the event items are present\n",
    "wait = WebDriverWait(driver, 10)\n",
    "event_items = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'eventitem')))\n",
    "\n",
    "# Initialize list to store event data\n",
    "events_data = []\n",
    "\n",
    "# Function to safely extract data from an event element\n",
    "def safe_extract(event, tag, class_name=None, get_text=True):\n",
    "    try:\n",
    "        if class_name:\n",
    "            element = event.find_element(By.CLASS_NAME, class_name)\n",
    "        else:\n",
    "            element = event.find_element(By.TAG_NAME, tag)\n",
    "        return element.text.strip() if get_text else element\n",
    "    except StaleElementReferenceException:\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Iterate over each event item\n",
    "for event in event_items:\n",
    "    retries = 3\n",
    "    while retries:\n",
    "        try:\n",
    "            # Extract event name and date\n",
    "            event_name = safe_extract(event, 'h1') or \"No Event Name\"\n",
    "            event_date = safe_extract(event, 'div', 'eventdate') or \"No Date\"\n",
    "            \n",
    "            # Extract link to event page for further details\n",
    "            more_info_link = safe_extract(event, 'a', 'button green right', get_text=False)\n",
    "            if more_info_link:\n",
    "                event_page_url = more_info_link.get_attribute('href')\n",
    "                \n",
    "                # Navigate to event detail page to extract description and location\n",
    "                driver.get(event_page_url)\n",
    "                \n",
    "                # Wait for the description and location to be visible\n",
    "                try:\n",
    "                    description_elem = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, 'copyContent'))\n",
    "                    )\n",
    "                    description = description_elem.text.strip() or \"No Description\"\n",
    "                except:\n",
    "                    description = \"No Description\"\n",
    "                \n",
    "                try:\n",
    "                    location_elem = driver.find_element(By.CLASS_NAME, 'eventlocation')\n",
    "                    location = location_elem.text.strip() or \"No Location\"\n",
    "                except:\n",
    "                    location = \"No Location\"\n",
    "                \n",
    "                # Return to the main event list page\n",
    "                driver.back()\n",
    "            else:\n",
    "                description = \"No Description\"\n",
    "                location = \"No Location\"\n",
    "            \n",
    "            # Store the event details in a dictionary\n",
    "            event_data = {\n",
    "                \"event_name\": event_name,\n",
    "                \"date\": event_date,\n",
    "                \"description\": description,\n",
    "                \"location\": location\n",
    "            }\n",
    "            \n",
    "            # Append to the list of events\n",
    "            events_data.append(event_data)\n",
    "            break  # Exit retry loop if successful\n",
    "        \n",
    "        except StaleElementReferenceException:\n",
    "            retries -= 1\n",
    "            if retries == 0:\n",
    "                print(f\"Failed to scrape event due to stale element after retries.\")\n",
    "            else:\n",
    "                print(f\"Retrying due to stale element reference... ({3 - retries}/3)\")\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('downtown_pittsburgh_events.json', 'w') as json_file:\n",
    "    json.dump(events_data, json_file, indent=4)\n",
    "\n",
    "print(\"Scraping completed. Event data has been saved to 'downtown_pittsburgh_events.json'.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "810237c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping events for 2024-10-28...\n",
      "Scraping events for 2024-10-29...\n",
      "Scraping events for 2024-10-30...\n",
      "Scraping events for 2024-10-31...\n",
      "Scraping events for 2024-11-01...\n",
      "Scraping events for 2024-11-02...\n",
      "Scraping events for 2024-11-03...\n",
      "Scraping events for 2024-11-04...\n",
      "Scraping events for 2024-11-05...\n",
      "Scraping events for 2024-11-06...\n",
      "Scraping events for 2024-11-07...\n",
      "Scraping events for 2024-11-08...\n",
      "Scraping events for 2024-11-09...\n",
      "Scraping events for 2024-11-10...\n",
      "Scraping events for 2024-11-11...\n",
      "Scraping events for 2024-11-12...\n",
      "Scraping events for 2024-11-13...\n",
      "Scraping events for 2024-11-14...\n",
      "Scraping events for 2024-11-15...\n",
      "Scraping events for 2024-11-16...\n",
      "Scraping events for 2024-11-17...\n",
      "Scraping events for 2024-11-18...\n",
      "Scraping events for 2024-11-19...\n",
      "Scraping events for 2024-11-20...\n",
      "Scraping events for 2024-11-21...\n",
      "Scraping events for 2024-11-22...\n",
      "Scraping events for 2024-11-23...\n",
      "Scraping events for 2024-11-24...\n",
      "Scraping events for 2024-11-25...\n",
      "Scraping events for 2024-11-26...\n",
      "Scraping events for 2024-11-27...\n",
      "Scraping events for 2024-11-28...\n",
      "Scraping events for 2024-11-29...\n",
      "Scraping events for 2024-11-30...\n",
      "Scraping events for 2024-12-01...\n",
      "Scraping events for 2024-12-02...\n",
      "Scraping events for 2024-12-03...\n",
      "Scraping events for 2024-12-04...\n",
      "Scraping events for 2024-12-05...\n",
      "Scraping events for 2024-12-06...\n",
      "Scraping events for 2024-12-07...\n",
      "Scraping events for 2024-12-08...\n",
      "Scraping events for 2024-12-09...\n",
      "Scraping events for 2024-12-10...\n",
      "Scraping events for 2024-12-11...\n",
      "Scraping events for 2024-12-12...\n",
      "Scraping events for 2024-12-13...\n",
      "Scraping events for 2024-12-14...\n",
      "Scraping events for 2024-12-15...\n",
      "Scraping events for 2024-12-16...\n",
      "Scraping events for 2024-12-17...\n",
      "Scraping events for 2024-12-18...\n",
      "Scraping events for 2024-12-19...\n",
      "Scraping events for 2024-12-20...\n",
      "Scraping events for 2024-12-21...\n",
      "Scraping events for 2024-12-22...\n",
      "Scraping events for 2024-12-23...\n",
      "Scraping events for 2024-12-24...\n",
      "Scraping events for 2024-12-25...\n",
      "Scraping events for 2024-12-26...\n",
      "Scraping events for 2024-12-27...\n",
      "Scraping events for 2024-12-28...\n",
      "Scraping events for 2024-12-29...\n",
      "Scraping events for 2024-12-30...\n",
      "Scraping events for 2024-12-31...\n",
      "Scraping events for 2025-01-01...\n",
      "Scraping events for 2025-01-02...\n",
      "Scraping events for 2025-01-03...\n",
      "Scraping events for 2025-01-04...\n",
      "Scraping events for 2025-01-05...\n",
      "Scraping events for 2025-01-06...\n",
      "Scraping events for 2025-01-07...\n",
      "Scraping events for 2025-01-08...\n",
      "Scraping events for 2025-01-09...\n",
      "Scraping events for 2025-01-10...\n",
      "Scraping events for 2025-01-11...\n",
      "Scraping events for 2025-01-12...\n",
      "Scraping events for 2025-01-13...\n",
      "Scraping events for 2025-01-14...\n",
      "Scraping events for 2025-01-15...\n",
      "Scraping events for 2025-01-16...\n",
      "Scraping events for 2025-01-17...\n",
      "Scraping events for 2025-01-18...\n",
      "Scraping events for 2025-01-19...\n",
      "Scraping events for 2025-01-20...\n",
      "Scraping events for 2025-01-21...\n",
      "Scraping events for 2025-01-22...\n",
      "Scraping events for 2025-01-23...\n",
      "Scraping events for 2025-01-24...\n",
      "Scraping events for 2025-01-25...\n",
      "Scraping events for 2025-01-26...\n",
      "Scraping events for 2025-01-27...\n",
      "Scraping events for 2025-01-28...\n",
      "Scraping events for 2025-01-29...\n",
      "Scraping events for 2025-01-30...\n",
      "Scraping events for 2025-01-31...\n",
      "Scraping events for 2025-02-01...\n",
      "Scraping events for 2025-02-02...\n",
      "Scraping events for 2025-02-03...\n",
      "Scraping events for 2025-02-04...\n",
      "Scraping events for 2025-02-05...\n",
      "Scraping events for 2025-02-06...\n",
      "Scraping events for 2025-02-07...\n",
      "Scraping events for 2025-02-08...\n",
      "Scraping events for 2025-02-09...\n",
      "Scraping events for 2025-02-10...\n",
      "Scraping events for 2025-02-11...\n",
      "Scraping events for 2025-02-12...\n",
      "Scraping events for 2025-02-13...\n",
      "Scraping events for 2025-02-14...\n",
      "Scraping events for 2025-02-15...\n",
      "Scraping events for 2025-02-16...\n",
      "Scraping events for 2025-02-17...\n",
      "Scraping events for 2025-02-18...\n",
      "Scraping events for 2025-02-19...\n",
      "Scraping events for 2025-02-20...\n",
      "Scraping events for 2025-02-21...\n",
      "Scraping events for 2025-02-22...\n",
      "Scraping events for 2025-02-23...\n",
      "Scraping events for 2025-02-24...\n",
      "Scraping events for 2025-02-25...\n",
      "Scraping events for 2025-02-26...\n",
      "Scraping events for 2025-02-27...\n",
      "Scraping events for 2025-02-28...\n",
      "Scraping events for 2025-03-01...\n",
      "Scraping events for 2025-03-02...\n",
      "Scraping events for 2025-03-03...\n",
      "Scraping events for 2025-03-04...\n",
      "Scraping events for 2025-03-05...\n",
      "Scraping events for 2025-03-06...\n",
      "Scraping events for 2025-03-07...\n",
      "Scraping events for 2025-03-08...\n",
      "Scraping events for 2025-03-09...\n",
      "Scraping events for 2025-03-10...\n",
      "Scraping events for 2025-03-11...\n",
      "Scraping events for 2025-03-12...\n",
      "Scraping events for 2025-03-13...\n",
      "Scraping events for 2025-03-14...\n",
      "Scraping events for 2025-03-15...\n",
      "Scraping events for 2025-03-16...\n",
      "Scraping events for 2025-03-17...\n",
      "Scraping events for 2025-03-18...\n",
      "Scraping events for 2025-03-19...\n",
      "Scraping events for 2025-03-20...\n",
      "Scraping events for 2025-03-21...\n",
      "Scraping events for 2025-03-22...\n",
      "Scraping events for 2025-03-23...\n",
      "Scraping events for 2025-03-24...\n",
      "Scraping events for 2025-03-25...\n",
      "Scraping events for 2025-03-26...\n",
      "Scraping events for 2025-03-27...\n",
      "Scraping events for 2025-03-28...\n",
      "Scraping events for 2025-03-29...\n",
      "Scraping events for 2025-03-30...\n",
      "Scraping events for 2025-03-31...\n",
      "Scraping events for 2025-04-01...\n",
      "Scraping events for 2025-04-02...\n",
      "Scraping events for 2025-04-03...\n",
      "Scraping events for 2025-04-04...\n",
      "Scraping events for 2025-04-05...\n",
      "Scraping events for 2025-04-06...\n",
      "Scraping events for 2025-04-07...\n",
      "Scraping events for 2025-04-08...\n",
      "Scraping events for 2025-04-09...\n",
      "Scraping events for 2025-04-10...\n",
      "Scraping events for 2025-04-11...\n",
      "Scraping events for 2025-04-12...\n",
      "Scraping events for 2025-04-13...\n",
      "Scraping events for 2025-04-14...\n",
      "Scraping events for 2025-04-15...\n",
      "Scraping events for 2025-04-16...\n",
      "Scraping events for 2025-04-17...\n",
      "Scraping events for 2025-04-18...\n",
      "Scraping events for 2025-04-19...\n",
      "Scraping events for 2025-04-20...\n",
      "Scraping events for 2025-04-21...\n",
      "Scraping events for 2025-04-22...\n",
      "Scraping events for 2025-04-23...\n",
      "Scraping events for 2025-04-24...\n",
      "Scraping events for 2025-04-25...\n",
      "Scraping events for 2025-04-26...\n",
      "Scraping events for 2025-04-27...\n",
      "Scraping events for 2025-04-28...\n",
      "Scraping events for 2025-04-29...\n",
      "Scraping events for 2025-04-30...\n",
      "Scraping events for 2025-05-01...\n",
      "Scraping events for 2025-05-02...\n",
      "Scraping events for 2025-05-03...\n",
      "Scraping events for 2025-05-04...\n",
      "Scraping events for 2025-05-05...\n",
      "Scraping events for 2025-05-06...\n",
      "Scraping events for 2025-05-07...\n",
      "Scraping events for 2025-05-08...\n",
      "Scraping events for 2025-05-09...\n",
      "Scraping events for 2025-05-10...\n",
      "Scraping events for 2025-05-11...\n",
      "Scraping events for 2025-05-12...\n",
      "Scraping events for 2025-05-13...\n",
      "Scraping events for 2025-05-14...\n",
      "Scraping events for 2025-05-15...\n",
      "Scraping events for 2025-05-16...\n",
      "Scraping events for 2025-05-17...\n",
      "Scraping events for 2025-05-18...\n",
      "Scraping events for 2025-05-19...\n",
      "Scraping events for 2025-05-20...\n",
      "Scraping events for 2025-05-21...\n",
      "Scraping events for 2025-05-22...\n",
      "Scraping events for 2025-05-23...\n",
      "Scraping events for 2025-05-24...\n",
      "Scraping events for 2025-05-25...\n",
      "Scraping events for 2025-05-26...\n",
      "Scraping events for 2025-05-27...\n",
      "Scraping events for 2025-05-28...\n",
      "Scraping events for 2025-05-29...\n",
      "Scraping events for 2025-05-30...\n",
      "Scraping events for 2025-05-31...\n",
      "Scraping events for 2025-06-01...\n",
      "Scraping events for 2025-06-02...\n",
      "Scraping events for 2025-06-03...\n",
      "Scraping events for 2025-06-04...\n",
      "Scraping events for 2025-06-05...\n",
      "Scraping events for 2025-06-06...\n",
      "Scraping events for 2025-06-07...\n",
      "Scraping events for 2025-06-08...\n",
      "Scraping events for 2025-06-09...\n",
      "Scraping events for 2025-06-10...\n",
      "Scraping events for 2025-06-11...\n",
      "Scraping events for 2025-06-12...\n",
      "Scraping events for 2025-06-13...\n",
      "Scraping events for 2025-06-14...\n",
      "Scraping events for 2025-06-15...\n",
      "Scraping events for 2025-06-16...\n",
      "Scraping events for 2025-06-17...\n",
      "Scraping events for 2025-06-18...\n",
      "Scraping events for 2025-06-19...\n",
      "Scraping events for 2025-06-20...\n",
      "Scraping events for 2025-06-21...\n",
      "Scraping events for 2025-06-22...\n",
      "Scraping events for 2025-06-23...\n",
      "Scraping events for 2025-06-24...\n",
      "Scraping events for 2025-06-25...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping events for 2025-06-26...\n",
      "Scraping events for 2025-06-27...\n",
      "Scraping events for 2025-06-28...\n",
      "Scraping events for 2025-06-29...\n",
      "Scraping events for 2025-06-30...\n",
      "Scraping events for 2025-07-01...\n",
      "Scraping events for 2025-07-02...\n",
      "Scraping events for 2025-07-03...\n",
      "Scraping events for 2025-07-04...\n",
      "Scraping events for 2025-07-05...\n",
      "Scraping events for 2025-07-06...\n",
      "Scraping events for 2025-07-07...\n",
      "Scraping events for 2025-07-08...\n",
      "Scraping events for 2025-07-09...\n",
      "Scraping events for 2025-07-10...\n",
      "Scraping events for 2025-07-11...\n",
      "Scraping events for 2025-07-12...\n",
      "Scraping events for 2025-07-13...\n",
      "Scraping events for 2025-07-14...\n",
      "Scraping events for 2025-07-15...\n",
      "Scraping events for 2025-07-16...\n",
      "Scraping events for 2025-07-17...\n",
      "Scraping events for 2025-07-18...\n",
      "Scraping events for 2025-07-19...\n",
      "Scraping events for 2025-07-20...\n",
      "Scraping events for 2025-07-21...\n",
      "Scraping events for 2025-07-22...\n",
      "Scraping events for 2025-07-23...\n",
      "Scraping events for 2025-07-24...\n",
      "Scraping events for 2025-07-25...\n",
      "Scraping events for 2025-07-26...\n",
      "Scraping events for 2025-07-27...\n",
      "Scraping events for 2025-07-28...\n",
      "Scraping events for 2025-07-29...\n",
      "Scraping events for 2025-07-30...\n",
      "Scraping events for 2025-07-31...\n",
      "Scraping events for 2025-08-01...\n",
      "Scraping events for 2025-08-02...\n",
      "Scraping events for 2025-08-03...\n",
      "Scraping events for 2025-08-04...\n",
      "Scraping events for 2025-08-05...\n",
      "Scraping events for 2025-08-06...\n",
      "Scraping events for 2025-08-07...\n",
      "Scraping events for 2025-08-08...\n",
      "Scraping events for 2025-08-09...\n",
      "Scraping events for 2025-08-10...\n",
      "Scraping events for 2025-08-11...\n",
      "Scraping events for 2025-08-12...\n",
      "Scraping events for 2025-08-13...\n",
      "Scraping events for 2025-08-14...\n",
      "Scraping events for 2025-08-15...\n",
      "Scraping events for 2025-08-16...\n",
      "Scraping events for 2025-08-17...\n",
      "Scraping events for 2025-08-18...\n",
      "Scraping events for 2025-08-19...\n",
      "Scraping events for 2025-08-20...\n",
      "Scraping events for 2025-08-21...\n",
      "Scraping events for 2025-08-22...\n",
      "Scraping events for 2025-08-23...\n",
      "Scraping events for 2025-08-24...\n",
      "Scraping events for 2025-08-25...\n",
      "Scraping events for 2025-08-26...\n",
      "Scraping events for 2025-08-27...\n",
      "Scraping events for 2025-08-28...\n",
      "Scraping events for 2025-08-29...\n",
      "Scraping events for 2025-08-30...\n",
      "Scraping events for 2025-08-31...\n",
      "Scraping events for 2025-09-01...\n",
      "Scraping events for 2025-09-02...\n",
      "Scraping events for 2025-09-03...\n",
      "Scraping events for 2025-09-04...\n",
      "Scraping events for 2025-09-05...\n",
      "Scraping events for 2025-09-06...\n",
      "Scraping events for 2025-09-07...\n",
      "Scraping events for 2025-09-08...\n",
      "Scraping events for 2025-09-09...\n",
      "Scraping events for 2025-09-10...\n",
      "Scraping events for 2025-09-11...\n",
      "Scraping events for 2025-09-12...\n",
      "Scraping events for 2025-09-13...\n",
      "Scraping events for 2025-09-14...\n",
      "Scraping events for 2025-09-15...\n",
      "Scraping events for 2025-09-16...\n",
      "Scraping events for 2025-09-17...\n",
      "Scraping events for 2025-09-18...\n",
      "Scraping events for 2025-09-19...\n",
      "Scraping events for 2025-09-20...\n",
      "Scraping events for 2025-09-21...\n",
      "Scraping events for 2025-09-22...\n",
      "Scraping events for 2025-09-23...\n",
      "Scraping events for 2025-09-24...\n",
      "Scraping events for 2025-09-25...\n",
      "Scraping events for 2025-09-26...\n",
      "Scraping events for 2025-09-27...\n",
      "Scraping events for 2025-09-28...\n",
      "Scraping events for 2025-09-29...\n",
      "Scraping events for 2025-09-30...\n",
      "Scraping events for 2025-10-01...\n",
      "Scraping events for 2025-10-02...\n",
      "Scraping events for 2025-10-03...\n",
      "Scraping events for 2025-10-04...\n",
      "Scraping events for 2025-10-05...\n",
      "Scraping events for 2025-10-06...\n",
      "Scraping events for 2025-10-07...\n",
      "Scraping events for 2025-10-08...\n",
      "Scraping events for 2025-10-09...\n",
      "Scraping events for 2025-10-10...\n",
      "Scraping events for 2025-10-11...\n",
      "Scraping events for 2025-10-12...\n",
      "Scraping events for 2025-10-13...\n",
      "Scraping events for 2025-10-14...\n",
      "Scraping events for 2025-10-15...\n",
      "Scraping events for 2025-10-16...\n",
      "Scraping events for 2025-10-17...\n",
      "Scraping events for 2025-10-18...\n",
      "Scraping events for 2025-10-19...\n",
      "Scraping events for 2025-10-20...\n",
      "Scraping events for 2025-10-21...\n",
      "Scraping events for 2025-10-22...\n",
      "Scraping events for 2025-10-23...\n",
      "Scraping events for 2025-10-24...\n",
      "Scraping events for 2025-10-25...\n",
      "Scraping events for 2025-10-26...\n",
      "Scraping events for 2025-10-27...\n",
      "Scraping events for 2025-10-28...\n",
      "Scraping completed. All event data has been saved to 'pittsburgh_events_one_year.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to scrape event data for a specific date\n",
    "def scrape_events_for_date(date):\n",
    "    # Format the date as required by the URL\n",
    "    day = date.day\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    url = f'https://downtownpittsburgh.com/events/?n=10&d={day}&m={month}&y={year}'\n",
    "    \n",
    "    # Request the page content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all event items within the relevant div\n",
    "    event_items = soup.select('div.eventitem')\n",
    "    \n",
    "    # Initialize list to store event data for this date\n",
    "    events_data = []\n",
    "    \n",
    "    # Iterate over each event item\n",
    "    for event in event_items:\n",
    "        # Extract event name\n",
    "        event_name = event.find('h1')\n",
    "        if event_name:\n",
    "            event_name = event_name.get_text(strip=True)\n",
    "        else:\n",
    "            event_name = \"No Event Name\"\n",
    "        \n",
    "        # Extract event date\n",
    "        event_date = event.find('div', class_='eventdate')\n",
    "        if event_date:\n",
    "            event_date = event_date.get_text(strip=True)\n",
    "        else:\n",
    "            event_date = \"No Date\"\n",
    "        \n",
    "        # Extract event description\n",
    "        description_tag = event.find(text=True, recursive=False)\n",
    "        if description_tag:\n",
    "            description = description_tag.strip()\n",
    "        else:\n",
    "            description = \"No Description\"\n",
    "        \n",
    "        # Extract link for the event page for further details (location)\n",
    "        more_info_link = event.find('a', class_='button green right')\n",
    "        if more_info_link and more_info_link['href']:\n",
    "            event_page_url = 'https://downtownpittsburgh.com' + more_info_link['href']\n",
    "            \n",
    "            # Fetch event page\n",
    "            event_page_response = requests.get(event_page_url)\n",
    "            event_page_soup = BeautifulSoup(event_page_response.content, 'html.parser')\n",
    "            \n",
    "            # Extract location from the event page\n",
    "            location_tag = event_page_soup.find('div', class_='eventlocation')\n",
    "            if location_tag:\n",
    "                location = location_tag.get_text(separator=\", \", strip=True)\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "        else:\n",
    "            location = \"No Location\"\n",
    "        \n",
    "        # Store the event details in a dictionary\n",
    "        event_data = {\n",
    "            \"event_name\": event_name,\n",
    "            \"date\": event_date,\n",
    "            \"description\": description,\n",
    "            \"location\": location\n",
    "        }\n",
    "        \n",
    "        # Append to the list of events for this date\n",
    "        events_data.append(event_data)\n",
    "    \n",
    "    return events_data\n",
    "\n",
    "# Start date: October 28, 2024\n",
    "start_date = datetime(2024, 10, 28)\n",
    "\n",
    "# End date: One year later\n",
    "end_date = datetime(2025, 10, 28)\n",
    "\n",
    "# Initialize a list to store all event data\n",
    "all_events_data = []\n",
    "\n",
    "# Iterate through all days from the start date to the end date\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    print(f\"Scraping events for {current_date.strftime('%Y-%m-%d')}...\")\n",
    "    \n",
    "    # Scrape the events for the current date\n",
    "    events_for_date = scrape_events_for_date(current_date)\n",
    "    \n",
    "    # Add the events to the overall list\n",
    "    all_events_data.extend(events_for_date)\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('pittsburgh_events_one_year.json', 'w') as json_file:\n",
    "    json.dump(all_events_data, json_file, indent=4)\n",
    "\n",
    "print(\"Scraping completed. All event data has been saved to 'pittsburgh_events_one_year.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f327bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping events for 2024-10-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/1qw339l937x_y3n17np39c5c0000gn/T/ipykernel_66279/2723800081.py:50: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  description_tag = event.find(text=True, recursive=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 events for 2024-10-28.\n",
      "Scraping events for 2024-10-29...\n",
      "Found 12 events for 2024-10-29.\n",
      "Scraping events for 2024-10-30...\n",
      "Found 13 events for 2024-10-30.\n",
      "Scraping events for 2024-10-31...\n",
      "Found 12 events for 2024-10-31.\n",
      "Scraping events for 2024-11-01...\n",
      "Found 10 events for 2024-11-01.\n",
      "Scraping events for 2024-11-02...\n",
      "Found 13 events for 2024-11-02.\n",
      "Scraping events for 2024-11-03...\n",
      "Found 11 events for 2024-11-03.\n",
      "Scraping events for 2024-11-04...\n",
      "Found 5 events for 2024-11-04.\n",
      "Scraping events for 2024-11-05...\n",
      "Found 5 events for 2024-11-05.\n",
      "Scraping events for 2024-11-06...\n",
      "Found 6 events for 2024-11-06.\n",
      "Scraping events for 2024-11-07...\n",
      "Found 7 events for 2024-11-07.\n",
      "Scraping events for 2024-11-08...\n",
      "Found 9 events for 2024-11-08.\n",
      "Scraping events for 2024-11-09...\n",
      "Found 10 events for 2024-11-09.\n",
      "Scraping events for 2024-11-10...\n",
      "Found 7 events for 2024-11-10.\n",
      "Scraping events for 2024-11-11...\n",
      "Found 4 events for 2024-11-11.\n",
      "Scraping events for 2024-11-12...\n",
      "Found 4 events for 2024-11-12.\n",
      "Scraping events for 2024-11-13...\n",
      "Found 5 events for 2024-11-13.\n",
      "Scraping events for 2024-11-14...\n",
      "Found 6 events for 2024-11-14.\n",
      "Scraping events for 2024-11-15...\n",
      "Found 7 events for 2024-11-15.\n",
      "Scraping events for 2024-11-16...\n",
      "Found 9 events for 2024-11-16.\n",
      "Scraping events for 2024-11-17...\n",
      "Found 5 events for 2024-11-17.\n",
      "Scraping events for 2024-11-18...\n",
      "Found 4 events for 2024-11-18.\n",
      "Scraping events for 2024-11-19...\n",
      "Found 5 events for 2024-11-19.\n",
      "Scraping events for 2024-11-20...\n",
      "Found 6 events for 2024-11-20.\n",
      "Scraping events for 2024-11-21...\n",
      "Found 6 events for 2024-11-21.\n",
      "Scraping events for 2024-11-22...\n",
      "Found 7 events for 2024-11-22.\n",
      "Scraping events for 2024-11-23...\n",
      "Found 9 events for 2024-11-23.\n",
      "Scraping events for 2024-11-24...\n",
      "Found 8 events for 2024-11-24.\n",
      "Scraping events for 2024-11-25...\n",
      "Found 5 events for 2024-11-25.\n",
      "Scraping events for 2024-11-26...\n",
      "Found 5 events for 2024-11-26.\n",
      "Scraping events for 2024-11-27...\n",
      "Found 6 events for 2024-11-27.\n",
      "Scraping events for 2024-11-28...\n",
      "Found 6 events for 2024-11-28.\n",
      "Scraping events for 2024-11-29...\n",
      "Found 6 events for 2024-11-29.\n",
      "Scraping events for 2024-11-30...\n",
      "Found 7 events for 2024-11-30.\n",
      "Scraping events for 2024-12-01...\n",
      "Found 5 events for 2024-12-01.\n",
      "Scraping events for 2024-12-02...\n",
      "Found 3 events for 2024-12-02.\n",
      "Scraping events for 2024-12-03...\n",
      "Found 3 events for 2024-12-03.\n",
      "Scraping events for 2024-12-04...\n",
      "Found 4 events for 2024-12-04.\n",
      "Scraping events for 2024-12-05...\n",
      "Found 4 events for 2024-12-05.\n",
      "Scraping events for 2024-12-06...\n",
      "Found 4 events for 2024-12-06.\n",
      "Scraping events for 2024-12-07...\n",
      "Found 5 events for 2024-12-07.\n",
      "Scraping events for 2024-12-08...\n",
      "Found 4 events for 2024-12-08.\n",
      "Scraping events for 2024-12-09...\n",
      "Found 3 events for 2024-12-09.\n",
      "Scraping events for 2024-12-10...\n",
      "Found 3 events for 2024-12-10.\n",
      "Scraping events for 2024-12-11...\n",
      "Found 5 events for 2024-12-11.\n",
      "Scraping events for 2024-12-12...\n",
      "Found 4 events for 2024-12-12.\n",
      "Scraping events for 2024-12-13...\n",
      "Found 5 events for 2024-12-13.\n",
      "Scraping events for 2024-12-14...\n",
      "Found 7 events for 2024-12-14.\n",
      "Scraping events for 2024-12-15...\n",
      "Found 4 events for 2024-12-15.\n",
      "Scraping events for 2024-12-16...\n",
      "Found 3 events for 2024-12-16.\n",
      "Scraping events for 2024-12-17...\n",
      "Found 4 events for 2024-12-17.\n",
      "Scraping events for 2024-12-18...\n",
      "Found 4 events for 2024-12-18.\n",
      "Scraping events for 2024-12-19...\n",
      "Found 4 events for 2024-12-19.\n",
      "Scraping events for 2024-12-20...\n",
      "Found 4 events for 2024-12-20.\n",
      "Scraping events for 2024-12-21...\n",
      "Found 5 events for 2024-12-21.\n",
      "Scraping events for 2024-12-22...\n",
      "Found 4 events for 2024-12-22.\n",
      "Scraping events for 2024-12-23...\n",
      "Found 3 events for 2024-12-23.\n",
      "Scraping events for 2024-12-24...\n",
      "Found 3 events for 2024-12-24.\n",
      "Scraping events for 2024-12-25...\n",
      "Found 4 events for 2024-12-25.\n",
      "Scraping events for 2024-12-26...\n",
      "Found 4 events for 2024-12-26.\n",
      "Scraping events for 2024-12-27...\n",
      "Found 4 events for 2024-12-27.\n",
      "Scraping events for 2024-12-28...\n",
      "Found 5 events for 2024-12-28.\n",
      "Scraping events for 2024-12-29...\n",
      "Found 4 events for 2024-12-29.\n",
      "Scraping events for 2024-12-30...\n",
      "Found 3 events for 2024-12-30.\n",
      "Scraping events for 2024-12-31...\n",
      "Found 3 events for 2024-12-31.\n",
      "Scraping events for 2025-01-01...\n",
      "Found 3 events for 2025-01-01.\n",
      "Scraping events for 2025-01-02...\n",
      "Found 3 events for 2025-01-02.\n",
      "Scraping events for 2025-01-03...\n",
      "Found 3 events for 2025-01-03.\n",
      "Scraping events for 2025-01-04...\n",
      "Found 3 events for 2025-01-04.\n",
      "Scraping events for 2025-01-05...\n",
      "Found 3 events for 2025-01-05.\n",
      "Scraping events for 2025-01-06...\n",
      "Found 1 events for 2025-01-06.\n",
      "Scraping events for 2025-01-07...\n",
      "Found 2 events for 2025-01-07.\n",
      "Scraping events for 2025-01-08...\n",
      "Found 2 events for 2025-01-08.\n",
      "Scraping events for 2025-01-09...\n",
      "Found 2 events for 2025-01-09.\n",
      "Scraping events for 2025-01-10...\n",
      "Found 2 events for 2025-01-10.\n",
      "Scraping events for 2025-01-11...\n",
      "Found 2 events for 2025-01-11.\n",
      "Scraping events for 2025-01-12...\n",
      "Found 2 events for 2025-01-12.\n",
      "Scraping events for 2025-01-13...\n",
      "No events found for 2025-01-13.\n",
      "Scraping events for 2025-01-14...\n",
      "No events found for 2025-01-14.\n",
      "Scraping events for 2025-01-15...\n",
      "No events found for 2025-01-15.\n",
      "Scraping events for 2025-01-16...\n",
      "No events found for 2025-01-16.\n",
      "Scraping events for 2025-01-17...\n",
      "No events found for 2025-01-17.\n",
      "Scraping events for 2025-01-18...\n",
      "No events found for 2025-01-18.\n",
      "Scraping events for 2025-01-19...\n",
      "No events found for 2025-01-19.\n",
      "Scraping events for 2025-01-20...\n",
      "No events found for 2025-01-20.\n",
      "Scraping events for 2025-01-21...\n",
      "No events found for 2025-01-21.\n",
      "Scraping events for 2025-01-22...\n",
      "No events found for 2025-01-22.\n",
      "Scraping events for 2025-01-23...\n",
      "No events found for 2025-01-23.\n",
      "Scraping events for 2025-01-24...\n",
      "No events found for 2025-01-24.\n",
      "Scraping events for 2025-01-25...\n",
      "No events found for 2025-01-25.\n",
      "Scraping events for 2025-01-26...\n",
      "No events found for 2025-01-26.\n",
      "Scraping events for 2025-01-27...\n",
      "No events found for 2025-01-27.\n",
      "Scraping events for 2025-01-28...\n",
      "No events found for 2025-01-28.\n",
      "Scraping events for 2025-01-29...\n",
      "No events found for 2025-01-29.\n",
      "Scraping events for 2025-01-30...\n",
      "No events found for 2025-01-30.\n",
      "Scraping events for 2025-01-31...\n",
      "No events found for 2025-01-31.\n",
      "Scraping events for 2025-02-01...\n",
      "No events found for 2025-02-01.\n",
      "Scraping events for 2025-02-02...\n",
      "No events found for 2025-02-02.\n",
      "Scraping events for 2025-02-03...\n",
      "No events found for 2025-02-03.\n",
      "Scraping events for 2025-02-04...\n",
      "No events found for 2025-02-04.\n",
      "Scraping events for 2025-02-05...\n",
      "No events found for 2025-02-05.\n",
      "Scraping events for 2025-02-06...\n",
      "No events found for 2025-02-06.\n",
      "Scraping events for 2025-02-07...\n",
      "No events found for 2025-02-07.\n",
      "Scraping events for 2025-02-08...\n",
      "No events found for 2025-02-08.\n",
      "Scraping events for 2025-02-09...\n",
      "No events found for 2025-02-09.\n",
      "Scraping events for 2025-02-10...\n",
      "No events found for 2025-02-10.\n",
      "Scraping events for 2025-02-11...\n",
      "No events found for 2025-02-11.\n",
      "Scraping events for 2025-02-12...\n",
      "No events found for 2025-02-12.\n",
      "Scraping events for 2025-02-13...\n",
      "No events found for 2025-02-13.\n",
      "Scraping events for 2025-02-14...\n",
      "No events found for 2025-02-14.\n",
      "Scraping events for 2025-02-15...\n",
      "No events found for 2025-02-15.\n",
      "Scraping events for 2025-02-16...\n",
      "No events found for 2025-02-16.\n",
      "Scraping events for 2025-02-17...\n",
      "No events found for 2025-02-17.\n",
      "Scraping events for 2025-02-18...\n",
      "No events found for 2025-02-18.\n",
      "Scraping events for 2025-02-19...\n",
      "No events found for 2025-02-19.\n",
      "Scraping events for 2025-02-20...\n",
      "No events found for 2025-02-20.\n",
      "Scraping events for 2025-02-21...\n",
      "No events found for 2025-02-21.\n",
      "Scraping events for 2025-02-22...\n",
      "No events found for 2025-02-22.\n",
      "Scraping events for 2025-02-23...\n",
      "No events found for 2025-02-23.\n",
      "Scraping events for 2025-02-24...\n",
      "No events found for 2025-02-24.\n",
      "Scraping events for 2025-02-25...\n",
      "No events found for 2025-02-25.\n",
      "Scraping events for 2025-02-26...\n",
      "No events found for 2025-02-26.\n",
      "Scraping events for 2025-02-27...\n",
      "No events found for 2025-02-27.\n",
      "Scraping events for 2025-02-28...\n",
      "No events found for 2025-02-28.\n",
      "Scraping events for 2025-03-01...\n",
      "No events found for 2025-03-01.\n",
      "Scraping events for 2025-03-02...\n",
      "No events found for 2025-03-02.\n",
      "Scraping events for 2025-03-03...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for 2025-03-03.\n",
      "Scraping events for 2025-03-04...\n",
      "No events found for 2025-03-04.\n",
      "Scraping events for 2025-03-05...\n",
      "No events found for 2025-03-05.\n",
      "Scraping events for 2025-03-06...\n",
      "No events found for 2025-03-06.\n",
      "Scraping events for 2025-03-07...\n",
      "No events found for 2025-03-07.\n",
      "Scraping events for 2025-03-08...\n",
      "No events found for 2025-03-08.\n",
      "Scraping events for 2025-03-09...\n",
      "No events found for 2025-03-09.\n",
      "Scraping events for 2025-03-10...\n",
      "No events found for 2025-03-10.\n",
      "Scraping events for 2025-03-11...\n",
      "No events found for 2025-03-11.\n",
      "Scraping events for 2025-03-12...\n",
      "No events found for 2025-03-12.\n",
      "Scraping events for 2025-03-13...\n",
      "No events found for 2025-03-13.\n",
      "Scraping events for 2025-03-14...\n",
      "No events found for 2025-03-14.\n",
      "Scraping events for 2025-03-15...\n",
      "No events found for 2025-03-15.\n",
      "Scraping events for 2025-03-16...\n",
      "No events found for 2025-03-16.\n",
      "Scraping events for 2025-03-17...\n",
      "No events found for 2025-03-17.\n",
      "Scraping events for 2025-03-18...\n",
      "No events found for 2025-03-18.\n",
      "Scraping events for 2025-03-19...\n",
      "No events found for 2025-03-19.\n",
      "Scraping events for 2025-03-20...\n",
      "No events found for 2025-03-20.\n",
      "Scraping events for 2025-03-21...\n",
      "No events found for 2025-03-21.\n",
      "Scraping events for 2025-03-22...\n",
      "No events found for 2025-03-22.\n",
      "Scraping events for 2025-03-23...\n",
      "No events found for 2025-03-23.\n",
      "Scraping events for 2025-03-24...\n",
      "No events found for 2025-03-24.\n",
      "Scraping events for 2025-03-25...\n",
      "No events found for 2025-03-25.\n",
      "Scraping events for 2025-03-26...\n",
      "No events found for 2025-03-26.\n",
      "Scraping events for 2025-03-27...\n",
      "No events found for 2025-03-27.\n",
      "Scraping events for 2025-03-28...\n",
      "No events found for 2025-03-28.\n",
      "Scraping events for 2025-03-29...\n",
      "No events found for 2025-03-29.\n",
      "Scraping events for 2025-03-30...\n",
      "No events found for 2025-03-30.\n",
      "Scraping events for 2025-03-31...\n",
      "No events found for 2025-03-31.\n",
      "Scraping events for 2025-04-01...\n",
      "No events found for 2025-04-01.\n",
      "Scraping events for 2025-04-02...\n",
      "No events found for 2025-04-02.\n",
      "Scraping events for 2025-04-03...\n",
      "No events found for 2025-04-03.\n",
      "Scraping events for 2025-04-04...\n",
      "No events found for 2025-04-04.\n",
      "Scraping events for 2025-04-05...\n",
      "No events found for 2025-04-05.\n",
      "Scraping events for 2025-04-06...\n",
      "No events found for 2025-04-06.\n",
      "Scraping events for 2025-04-07...\n",
      "No events found for 2025-04-07.\n",
      "Scraping events for 2025-04-08...\n",
      "No events found for 2025-04-08.\n",
      "Scraping events for 2025-04-09...\n",
      "No events found for 2025-04-09.\n",
      "Scraping events for 2025-04-10...\n",
      "No events found for 2025-04-10.\n",
      "Scraping events for 2025-04-11...\n",
      "No events found for 2025-04-11.\n",
      "Scraping events for 2025-04-12...\n",
      "No events found for 2025-04-12.\n",
      "Scraping events for 2025-04-13...\n",
      "No events found for 2025-04-13.\n",
      "Scraping events for 2025-04-14...\n",
      "No events found for 2025-04-14.\n",
      "Scraping events for 2025-04-15...\n",
      "No events found for 2025-04-15.\n",
      "Scraping events for 2025-04-16...\n",
      "No events found for 2025-04-16.\n",
      "Scraping events for 2025-04-17...\n",
      "No events found for 2025-04-17.\n",
      "Scraping events for 2025-04-18...\n",
      "No events found for 2025-04-18.\n",
      "Scraping events for 2025-04-19...\n",
      "No events found for 2025-04-19.\n",
      "Scraping events for 2025-04-20...\n",
      "No events found for 2025-04-20.\n",
      "Scraping events for 2025-04-21...\n",
      "No events found for 2025-04-21.\n",
      "Scraping events for 2025-04-22...\n",
      "No events found for 2025-04-22.\n",
      "Scraping events for 2025-04-23...\n",
      "No events found for 2025-04-23.\n",
      "Scraping events for 2025-04-24...\n",
      "No events found for 2025-04-24.\n",
      "Scraping events for 2025-04-25...\n",
      "No events found for 2025-04-25.\n",
      "Scraping events for 2025-04-26...\n",
      "No events found for 2025-04-26.\n",
      "Scraping events for 2025-04-27...\n",
      "No events found for 2025-04-27.\n",
      "Scraping events for 2025-04-28...\n",
      "No events found for 2025-04-28.\n",
      "Scraping events for 2025-04-29...\n",
      "No events found for 2025-04-29.\n",
      "Scraping events for 2025-04-30...\n",
      "No events found for 2025-04-30.\n",
      "Scraping events for 2025-05-01...\n",
      "No events found for 2025-05-01.\n",
      "Scraping events for 2025-05-02...\n",
      "No events found for 2025-05-02.\n",
      "Scraping events for 2025-05-03...\n",
      "No events found for 2025-05-03.\n",
      "Scraping events for 2025-05-04...\n",
      "No events found for 2025-05-04.\n",
      "Scraping events for 2025-05-05...\n",
      "No events found for 2025-05-05.\n",
      "Scraping events for 2025-05-06...\n",
      "No events found for 2025-05-06.\n",
      "Scraping events for 2025-05-07...\n",
      "No events found for 2025-05-07.\n",
      "Scraping events for 2025-05-08...\n",
      "No events found for 2025-05-08.\n",
      "Scraping events for 2025-05-09...\n",
      "No events found for 2025-05-09.\n",
      "Scraping events for 2025-05-10...\n",
      "No events found for 2025-05-10.\n",
      "Scraping events for 2025-05-11...\n",
      "No events found for 2025-05-11.\n",
      "Scraping events for 2025-05-12...\n",
      "No events found for 2025-05-12.\n",
      "Scraping events for 2025-05-13...\n",
      "No events found for 2025-05-13.\n",
      "Scraping events for 2025-05-14...\n",
      "No events found for 2025-05-14.\n",
      "Scraping events for 2025-05-15...\n",
      "No events found for 2025-05-15.\n",
      "Scraping events for 2025-05-16...\n",
      "No events found for 2025-05-16.\n",
      "Scraping events for 2025-05-17...\n",
      "No events found for 2025-05-17.\n",
      "Scraping events for 2025-05-18...\n",
      "No events found for 2025-05-18.\n",
      "Scraping events for 2025-05-19...\n",
      "No events found for 2025-05-19.\n",
      "Scraping events for 2025-05-20...\n",
      "No events found for 2025-05-20.\n",
      "Scraping events for 2025-05-21...\n",
      "No events found for 2025-05-21.\n",
      "Scraping events for 2025-05-22...\n",
      "No events found for 2025-05-22.\n",
      "Scraping events for 2025-05-23...\n",
      "No events found for 2025-05-23.\n",
      "Scraping events for 2025-05-24...\n",
      "No events found for 2025-05-24.\n",
      "Scraping events for 2025-05-25...\n",
      "No events found for 2025-05-25.\n",
      "Scraping events for 2025-05-26...\n",
      "No events found for 2025-05-26.\n",
      "Scraping events for 2025-05-27...\n",
      "No events found for 2025-05-27.\n",
      "Scraping events for 2025-05-28...\n",
      "No events found for 2025-05-28.\n",
      "Scraping events for 2025-05-29...\n",
      "No events found for 2025-05-29.\n",
      "Scraping events for 2025-05-30...\n",
      "No events found for 2025-05-30.\n",
      "Scraping events for 2025-05-31...\n",
      "No events found for 2025-05-31.\n",
      "Scraping events for 2025-06-01...\n",
      "No events found for 2025-06-01.\n",
      "Scraping events for 2025-06-02...\n",
      "No events found for 2025-06-02.\n",
      "Scraping events for 2025-06-03...\n",
      "No events found for 2025-06-03.\n",
      "Scraping events for 2025-06-04...\n",
      "No events found for 2025-06-04.\n",
      "Scraping events for 2025-06-05...\n",
      "No events found for 2025-06-05.\n",
      "Scraping events for 2025-06-06...\n",
      "No events found for 2025-06-06.\n",
      "Scraping events for 2025-06-07...\n",
      "No events found for 2025-06-07.\n",
      "Scraping events for 2025-06-08...\n",
      "No events found for 2025-06-08.\n",
      "Scraping events for 2025-06-09...\n",
      "No events found for 2025-06-09.\n",
      "Scraping events for 2025-06-10...\n",
      "No events found for 2025-06-10.\n",
      "Scraping events for 2025-06-11...\n",
      "No events found for 2025-06-11.\n",
      "Scraping events for 2025-06-12...\n",
      "No events found for 2025-06-12.\n",
      "Scraping events for 2025-06-13...\n",
      "No events found for 2025-06-13.\n",
      "Scraping events for 2025-06-14...\n",
      "No events found for 2025-06-14.\n",
      "Scraping events for 2025-06-15...\n",
      "No events found for 2025-06-15.\n",
      "Scraping events for 2025-06-16...\n",
      "No events found for 2025-06-16.\n",
      "Scraping events for 2025-06-17...\n",
      "No events found for 2025-06-17.\n",
      "Scraping events for 2025-06-18...\n",
      "No events found for 2025-06-18.\n",
      "Scraping events for 2025-06-19...\n",
      "No events found for 2025-06-19.\n",
      "Scraping events for 2025-06-20...\n",
      "No events found for 2025-06-20.\n",
      "Scraping events for 2025-06-21...\n",
      "No events found for 2025-06-21.\n",
      "Scraping events for 2025-06-22...\n",
      "No events found for 2025-06-22.\n",
      "Scraping events for 2025-06-23...\n",
      "No events found for 2025-06-23.\n",
      "Scraping events for 2025-06-24...\n",
      "No events found for 2025-06-24.\n",
      "Scraping events for 2025-06-25...\n",
      "No events found for 2025-06-25.\n",
      "Scraping events for 2025-06-26...\n",
      "No events found for 2025-06-26.\n",
      "Scraping events for 2025-06-27...\n",
      "No events found for 2025-06-27.\n",
      "Scraping events for 2025-06-28...\n",
      "No events found for 2025-06-28.\n",
      "Scraping events for 2025-06-29...\n",
      "No events found for 2025-06-29.\n",
      "Scraping events for 2025-06-30...\n",
      "No events found for 2025-06-30.\n",
      "Scraping events for 2025-07-01...\n",
      "No events found for 2025-07-01.\n",
      "Scraping events for 2025-07-02...\n",
      "No events found for 2025-07-02.\n",
      "Scraping events for 2025-07-03...\n",
      "No events found for 2025-07-03.\n",
      "Scraping events for 2025-07-04...\n",
      "No events found for 2025-07-04.\n",
      "Scraping events for 2025-07-05...\n",
      "No events found for 2025-07-05.\n",
      "Scraping events for 2025-07-06...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found for 2025-07-06.\n",
      "Scraping events for 2025-07-07...\n",
      "No events found for 2025-07-07.\n",
      "Scraping events for 2025-07-08...\n",
      "No events found for 2025-07-08.\n",
      "Scraping events for 2025-07-09...\n",
      "No events found for 2025-07-09.\n",
      "Scraping events for 2025-07-10...\n",
      "No events found for 2025-07-10.\n",
      "Scraping events for 2025-07-11...\n",
      "No events found for 2025-07-11.\n",
      "Scraping events for 2025-07-12...\n",
      "No events found for 2025-07-12.\n",
      "Scraping events for 2025-07-13...\n",
      "No events found for 2025-07-13.\n",
      "Scraping events for 2025-07-14...\n",
      "No events found for 2025-07-14.\n",
      "Scraping events for 2025-07-15...\n",
      "No events found for 2025-07-15.\n",
      "Scraping events for 2025-07-16...\n",
      "No events found for 2025-07-16.\n",
      "Scraping events for 2025-07-17...\n",
      "No events found for 2025-07-17.\n",
      "Scraping events for 2025-07-18...\n",
      "No events found for 2025-07-18.\n",
      "Scraping events for 2025-07-19...\n",
      "No events found for 2025-07-19.\n",
      "Scraping events for 2025-07-20...\n",
      "No events found for 2025-07-20.\n",
      "Scraping events for 2025-07-21...\n",
      "No events found for 2025-07-21.\n",
      "Scraping events for 2025-07-22...\n",
      "No events found for 2025-07-22.\n",
      "Scraping events for 2025-07-23...\n",
      "No events found for 2025-07-23.\n",
      "Scraping events for 2025-07-24...\n",
      "No events found for 2025-07-24.\n",
      "Scraping events for 2025-07-25...\n",
      "No events found for 2025-07-25.\n",
      "Scraping events for 2025-07-26...\n",
      "No events found for 2025-07-26.\n",
      "Scraping events for 2025-07-27...\n",
      "No events found for 2025-07-27.\n",
      "Scraping events for 2025-07-28...\n",
      "No events found for 2025-07-28.\n",
      "Scraping events for 2025-07-29...\n",
      "No events found for 2025-07-29.\n",
      "Scraping events for 2025-07-30...\n",
      "No events found for 2025-07-30.\n",
      "Scraping events for 2025-07-31...\n",
      "No events found for 2025-07-31.\n",
      "Scraping events for 2025-08-01...\n",
      "No events found for 2025-08-01.\n",
      "Scraping events for 2025-08-02...\n",
      "No events found for 2025-08-02.\n",
      "Scraping events for 2025-08-03...\n",
      "No events found for 2025-08-03.\n",
      "Scraping events for 2025-08-04...\n",
      "No events found for 2025-08-04.\n",
      "Scraping events for 2025-08-05...\n",
      "No events found for 2025-08-05.\n",
      "Scraping events for 2025-08-06...\n",
      "No events found for 2025-08-06.\n",
      "Scraping events for 2025-08-07...\n",
      "No events found for 2025-08-07.\n",
      "Scraping events for 2025-08-08...\n",
      "No events found for 2025-08-08.\n",
      "Scraping events for 2025-08-09...\n",
      "No events found for 2025-08-09.\n",
      "Scraping events for 2025-08-10...\n",
      "No events found for 2025-08-10.\n",
      "Scraping events for 2025-08-11...\n",
      "No events found for 2025-08-11.\n",
      "Scraping events for 2025-08-12...\n",
      "No events found for 2025-08-12.\n",
      "Scraping events for 2025-08-13...\n",
      "No events found for 2025-08-13.\n",
      "Scraping events for 2025-08-14...\n",
      "No events found for 2025-08-14.\n",
      "Scraping events for 2025-08-15...\n",
      "No events found for 2025-08-15.\n",
      "Scraping events for 2025-08-16...\n",
      "No events found for 2025-08-16.\n",
      "Scraping events for 2025-08-17...\n",
      "No events found for 2025-08-17.\n",
      "Scraping events for 2025-08-18...\n",
      "No events found for 2025-08-18.\n",
      "Scraping events for 2025-08-19...\n",
      "No events found for 2025-08-19.\n",
      "Scraping events for 2025-08-20...\n",
      "No events found for 2025-08-20.\n",
      "Scraping events for 2025-08-21...\n",
      "No events found for 2025-08-21.\n",
      "Scraping events for 2025-08-22...\n",
      "No events found for 2025-08-22.\n",
      "Scraping events for 2025-08-23...\n",
      "No events found for 2025-08-23.\n",
      "Scraping events for 2025-08-24...\n",
      "No events found for 2025-08-24.\n",
      "Scraping events for 2025-08-25...\n",
      "No events found for 2025-08-25.\n",
      "Scraping events for 2025-08-26...\n",
      "No events found for 2025-08-26.\n",
      "Scraping events for 2025-08-27...\n",
      "No events found for 2025-08-27.\n",
      "Scraping events for 2025-08-28...\n",
      "No events found for 2025-08-28.\n",
      "Scraping events for 2025-08-29...\n",
      "No events found for 2025-08-29.\n",
      "Scraping events for 2025-08-30...\n",
      "No events found for 2025-08-30.\n",
      "Scraping events for 2025-08-31...\n",
      "No events found for 2025-08-31.\n",
      "Scraping events for 2025-09-01...\n",
      "No events found for 2025-09-01.\n",
      "Scraping events for 2025-09-02...\n",
      "No events found for 2025-09-02.\n",
      "Scraping events for 2025-09-03...\n",
      "No events found for 2025-09-03.\n",
      "Scraping events for 2025-09-04...\n",
      "No events found for 2025-09-04.\n",
      "Scraping events for 2025-09-05...\n",
      "No events found for 2025-09-05.\n",
      "Scraping events for 2025-09-06...\n",
      "No events found for 2025-09-06.\n",
      "Scraping events for 2025-09-07...\n",
      "No events found for 2025-09-07.\n",
      "Scraping events for 2025-09-08...\n",
      "No events found for 2025-09-08.\n",
      "Scraping events for 2025-09-09...\n",
      "No events found for 2025-09-09.\n",
      "Scraping events for 2025-09-10...\n",
      "No events found for 2025-09-10.\n",
      "Scraping events for 2025-09-11...\n",
      "No events found for 2025-09-11.\n",
      "Scraping events for 2025-09-12...\n",
      "No events found for 2025-09-12.\n",
      "Scraping events for 2025-09-13...\n",
      "No events found for 2025-09-13.\n",
      "Scraping events for 2025-09-14...\n",
      "No events found for 2025-09-14.\n",
      "Scraping events for 2025-09-15...\n",
      "No events found for 2025-09-15.\n",
      "Scraping events for 2025-09-16...\n",
      "No events found for 2025-09-16.\n",
      "Scraping events for 2025-09-17...\n",
      "No events found for 2025-09-17.\n",
      "Scraping events for 2025-09-18...\n",
      "No events found for 2025-09-18.\n",
      "Scraping events for 2025-09-19...\n",
      "No events found for 2025-09-19.\n",
      "Scraping events for 2025-09-20...\n",
      "No events found for 2025-09-20.\n",
      "Scraping events for 2025-09-21...\n",
      "No events found for 2025-09-21.\n",
      "Scraping events for 2025-09-22...\n",
      "No events found for 2025-09-22.\n",
      "Scraping events for 2025-09-23...\n",
      "No events found for 2025-09-23.\n",
      "Scraping events for 2025-09-24...\n",
      "No events found for 2025-09-24.\n",
      "Scraping events for 2025-09-25...\n",
      "No events found for 2025-09-25.\n",
      "Scraping events for 2025-09-26...\n",
      "No events found for 2025-09-26.\n",
      "Scraping events for 2025-09-27...\n",
      "No events found for 2025-09-27.\n",
      "Scraping events for 2025-09-28...\n",
      "No events found for 2025-09-28.\n",
      "Scraping events for 2025-09-29...\n",
      "No events found for 2025-09-29.\n",
      "Scraping events for 2025-09-30...\n",
      "No events found for 2025-09-30.\n",
      "Scraping events for 2025-10-01...\n",
      "No events found for 2025-10-01.\n",
      "Scraping events for 2025-10-02...\n",
      "No events found for 2025-10-02.\n",
      "Scraping events for 2025-10-03...\n",
      "No events found for 2025-10-03.\n",
      "Scraping events for 2025-10-04...\n",
      "No events found for 2025-10-04.\n",
      "Scraping events for 2025-10-05...\n",
      "No events found for 2025-10-05.\n",
      "Scraping events for 2025-10-06...\n",
      "No events found for 2025-10-06.\n",
      "Scraping events for 2025-10-07...\n",
      "No events found for 2025-10-07.\n",
      "Scraping events for 2025-10-08...\n",
      "No events found for 2025-10-08.\n",
      "Scraping events for 2025-10-09...\n",
      "No events found for 2025-10-09.\n",
      "Scraping events for 2025-10-10...\n",
      "No events found for 2025-10-10.\n",
      "Scraping events for 2025-10-11...\n",
      "No events found for 2025-10-11.\n",
      "Scraping events for 2025-10-12...\n",
      "No events found for 2025-10-12.\n",
      "Scraping events for 2025-10-13...\n",
      "No events found for 2025-10-13.\n",
      "Scraping events for 2025-10-14...\n",
      "No events found for 2025-10-14.\n",
      "Scraping events for 2025-10-15...\n",
      "No events found for 2025-10-15.\n",
      "Scraping events for 2025-10-16...\n",
      "No events found for 2025-10-16.\n",
      "Scraping events for 2025-10-17...\n",
      "No events found for 2025-10-17.\n",
      "Scraping events for 2025-10-18...\n",
      "No events found for 2025-10-18.\n",
      "Scraping events for 2025-10-19...\n",
      "No events found for 2025-10-19.\n",
      "Scraping events for 2025-10-20...\n",
      "No events found for 2025-10-20.\n",
      "Scraping events for 2025-10-21...\n",
      "No events found for 2025-10-21.\n",
      "Scraping events for 2025-10-22...\n",
      "No events found for 2025-10-22.\n",
      "Scraping events for 2025-10-23...\n",
      "No events found for 2025-10-23.\n",
      "Scraping events for 2025-10-24...\n",
      "No events found for 2025-10-24.\n",
      "Scraping events for 2025-10-25...\n",
      "No events found for 2025-10-25.\n",
      "Scraping events for 2025-10-26...\n",
      "No events found for 2025-10-26.\n",
      "Scraping events for 2025-10-27...\n",
      "No events found for 2025-10-27.\n",
      "Scraping events for 2025-10-28...\n",
      "No events found for 2025-10-28.\n",
      "Scraping completed. Event data has been saved to 'pittsburgh_events_one_year.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to scrape event data for a specific date\n",
    "def scrape_events_for_date(date):\n",
    "    # Format the date as required by the URL (n is the month, d is the day, y is the year)\n",
    "    day = date.day\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    url = f'https://downtownpittsburgh.com/events/?n={month}&d={day}&y={year}'\n",
    "    \n",
    "    # Request the page content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data for {date.strftime('%Y-%m-%d')}, status code: {response.status_code}\")\n",
    "        return []  # Return an empty list if there's an issue with the request\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all event items within the relevant div\n",
    "    event_items = soup.select('div.eventitem')\n",
    "    \n",
    "    if not event_items:\n",
    "        print(f\"No events found for {date.strftime('%Y-%m-%d')}.\")\n",
    "        return []\n",
    "    \n",
    "    # Initialize list to store event data for this date\n",
    "    events_data = []\n",
    "    \n",
    "    # Iterate over each event item\n",
    "    for event in event_items:\n",
    "        # Extract event name\n",
    "        event_name = event.find('h1')\n",
    "        if event_name:\n",
    "            event_name = event_name.get_text(strip=True)\n",
    "        else:\n",
    "            event_name = \"No Event Name\"\n",
    "        \n",
    "        # Extract event date\n",
    "        event_date = event.find('div', class_='eventdate')\n",
    "        if event_date:\n",
    "            event_date = event_date.get_text(strip=True)\n",
    "        else:\n",
    "            event_date = \"No Date\"\n",
    "        \n",
    "        # Extract event description\n",
    "        description_tag = event.find(text=True, recursive=False)\n",
    "        if description_tag:\n",
    "            description = description_tag.strip()\n",
    "        else:\n",
    "            description = \"No Description\"\n",
    "        \n",
    "        # Extract link for the event page for further details (location)\n",
    "        more_info_link = event.find('a', class_='button green right')\n",
    "        if more_info_link and more_info_link['href']:\n",
    "            event_page_url = 'https://downtownpittsburgh.com' + more_info_link['href']\n",
    "            \n",
    "            # Fetch event page\n",
    "            event_page_response = requests.get(event_page_url)\n",
    "            event_page_soup = BeautifulSoup(event_page_response.content, 'html.parser')\n",
    "            \n",
    "            # Extract location from the event page\n",
    "            location_tag = event_page_soup.find('div', class_='eventlocation')\n",
    "            if location_tag:\n",
    "                location = location_tag.get_text(separator=\", \", strip=True)\n",
    "            else:\n",
    "                location = \"No Location\"\n",
    "        else:\n",
    "            location = \"No Location\"\n",
    "        \n",
    "        # Store the event details in a dictionary\n",
    "        event_data = {\n",
    "            \"event_name\": event_name,\n",
    "            \"date\": event_date,\n",
    "            \"description\": description,\n",
    "            \"location\": location\n",
    "        }\n",
    "        \n",
    "        # Append to the list of events for this date\n",
    "        events_data.append(event_data)\n",
    "    \n",
    "    print(f\"Found {len(events_data)} events for {date.strftime('%Y-%m-%d')}.\")\n",
    "    return events_data\n",
    "\n",
    "# Start date: October 28, 2024\n",
    "start_date = datetime(2024, 10, 28)\n",
    "\n",
    "# End date: One year later\n",
    "end_date = datetime(2025, 10, 28)\n",
    "\n",
    "# Initialize a list to store all event data\n",
    "all_events_data = []\n",
    "\n",
    "# Iterate through all days from the start date to the end date\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    print(f\"Scraping events for {current_date.strftime('%Y-%m-%d')}...\")\n",
    "    \n",
    "    # Scrape the events for the current date\n",
    "    events_for_date = scrape_events_for_date(current_date)\n",
    "    \n",
    "    # Add the events to the overall list\n",
    "    if events_for_date:\n",
    "        all_events_data.extend(events_for_date)\n",
    "    \n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Check if any events were collected before saving to the file\n",
    "if all_events_data:\n",
    "    # Save the data to a JSON file\n",
    "    with open('pittsburgh_events_one_year.json', 'w') as json_file:\n",
    "        json.dump(all_events_data, json_file, indent=4)\n",
    "    print(\"Scraping completed. Event data has been saved to 'pittsburgh_events_one_year.json'.\")\n",
    "else:\n",
    "    print(\"No events were scraped for the entire year.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04378320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
